# -*- coding: utf-8 -*-
"""
å†…ç½®å·¥å…·åŒ…è£…å™¨
å°†å†…ç½®å·¥å…·åŒ…è£…æˆBaseToolæ¥å£
"""
from typing import Dict, Any, List, Optional
from tools.base_tool import BaseTool
from utils.log_helper import get_logger
import json

logger = get_logger("builtin_tools")

# å¯¼å…¥å†…ç½®å·¥å…·
try:
    from tools.code_interpreter import code_interpreter_agent
    CODE_INTERPRETER_AVAILABLE = True
except ImportError:
    try:
        from genie_tool.tool.code_interpreter import code_interpreter_agent
        CODE_INTERPRETER_AVAILABLE = True
    except ImportError:
        CODE_INTERPRETER_AVAILABLE = False
        logger.warning("code_interpreter å·¥å…·ä¸å¯ç”¨")

try:
    from tools.deepsearch import DeepSearch
    DEEPSEARCH_AVAILABLE = True
except ImportError:
    try:
        from genie_tool.tool.deepsearch import DeepSearch
        DEEPSEARCH_AVAILABLE = True
    except ImportError:
        DEEPSEARCH_AVAILABLE = False
        logger.warning("deepsearch å·¥å…·ä¸å¯ç”¨")

try:
    from tools.report import report
    REPORT_AVAILABLE = True
except ImportError:
    try:
        from genie_tool.tool.report import report
        REPORT_AVAILABLE = True
    except ImportError:
        REPORT_AVAILABLE = False
        logger.warning("report å·¥å…·ä¸å¯ç”¨")


class CodeInterpreterTool(BaseTool):
    """ä»£ç è§£é‡Šå™¨å·¥å…·"""
    
    def __init__(self):
        super().__init__(
            name="code_interpreter",
            description="ä»£ç è§£é‡Šå™¨å·¥å…·ï¼Œå¯ä»¥æ‰§è¡ŒPythonä»£ç å¹¶å¤„ç†æ–‡ä»¶",
            container_type=BaseTool.CONTAINER_TYPE_FILE,  # ç»‘å®šæ–‡ä»¶å®¹å™¨
            container_config={
                "workspace_dir": "code_output",
                "allowed_extensions": [".py", ".txt", ".md", ".csv", ".xlsx", ".json"]
            }
        )
    
    async def execute(self, parameters: Dict[str, Any]) -> Any:
        """æ‰§è¡Œä»£ç è§£é‡Šå™¨"""
        if not CODE_INTERPRETER_AVAILABLE:
            raise RuntimeError("ä»£ç è§£é‡Šå™¨å·¥å…·ä¸å¯ç”¨")
        
        task = parameters.get("task", "")
        file_names = parameters.get("file_names", [])
        max_file_abstract_size = parameters.get("max_file_abstract_size", 2000)
        max_tokens = parameters.get("max_tokens", 32000)
        request_id = parameters.get("request_id", "")
        stream = parameters.get("stream", False)
        
        results = []
        async for chunk in code_interpreter_agent(
            task=task,
            file_names=file_names,
            max_file_abstract_size=max_file_abstract_size,
            max_tokens=max_tokens,
            request_id=request_id,
            stream=stream
        ):
            if stream:
                results.append(chunk)
            else:
                results.append(chunk)
        
        return results if stream else (results[0] if results else None)
    
    def get_parameters_schema(self) -> Dict[str, Any]:
        """è·å–å‚æ•°æ¨¡å¼"""
        return {
            "type": "object",
            "properties": {
                "task": {
                    "type": "string",
                    "description": "è¦æ‰§è¡Œçš„ä»»åŠ¡æè¿°"
                },
                "file_names": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "è¦å¤„ç†çš„æ–‡ä»¶ååˆ—è¡¨",
                    "default": []
                },
                "max_file_abstract_size": {
                    "type": "integer",
                    "description": "æ–‡ä»¶æ‘˜è¦æœ€å¤§å¤§å°",
                    "default": 2000
                },
                "max_tokens": {
                    "type": "integer",
                    "description": "æœ€å¤§tokenæ•°",
                    "default": 32000
                },
                "request_id": {
                    "type": "string",
                    "description": "è¯·æ±‚ID",
                    "default": ""
                },
                "stream": {
                    "type": "boolean",
                    "description": "æ˜¯å¦æµå¼è¿”å›",
                    "default": False
                }
            },
            "required": ["task"]
        }


class DeepSearchTool(BaseTool):
    """æ·±åº¦æœç´¢å·¥å…·"""
    
    def __init__(self):
        super().__init__(
            name="deepsearch",
            description="æ·±åº¦æœç´¢å·¥å…·ï¼Œå¯ä»¥è¿›è¡Œå¤šè½®æœç´¢å’Œæ¨ç†",
            container_type=BaseTool.CONTAINER_TYPE_BROWSER,  # ç»‘å®šæµè§ˆå®¹å™¨
            container_config={
                "browser_type": "headless",
                "timeout": 30
            }
        )
        self._search_instance = None
    
    def _get_search_instance(self):
        """è·å–æœç´¢å®ä¾‹"""
        if self._search_instance is None:
            # åŠ¨æ€å¯¼å…¥ï¼Œç¡®ä¿ä¾èµ–å¯ç”¨
            try:
                from tools.deepsearch import DeepSearch
            except ImportError:
                try:
                    from genie_tool.tool.deepsearch import DeepSearch
                except ImportError:
                    raise RuntimeError("æ·±åº¦æœç´¢å·¥å…·ä¸å¯ç”¨ï¼šç¼ºå°‘ä¾èµ– json-repairï¼Œè¯·è¿è¡Œ 'pip install json-repair' å®‰è£…")
            self._search_instance = DeepSearch()
        return self._search_instance
    
    async def execute(self, parameters: Dict[str, Any]) -> Any:
        """æ‰§è¡Œæ·±åº¦æœç´¢"""
        # åŠ¨æ€æ£€æŸ¥å·¥å…·æ˜¯å¦å¯ç”¨ï¼Œè€Œä¸æ˜¯ä¾èµ–æ¨¡å—çº§åˆ«çš„æ ‡å¿—
        # è¿™æ ·å³ä½¿æœåŠ¡å™¨åœ¨å®‰è£…ä¾èµ–å‰å¯åŠ¨ï¼Œä¹Ÿèƒ½åœ¨ä¾èµ–å®‰è£…åæ­£å¸¸å·¥ä½œ
        try:
            from tools.deepsearch import DeepSearch
        except ImportError:
            try:
                from genie_tool.tool.deepsearch import DeepSearch
            except ImportError:
                raise RuntimeError("æ·±åº¦æœç´¢å·¥å…·ä¸å¯ç”¨ï¼šç¼ºå°‘ä¾èµ– json-repairï¼Œè¯·è¿è¡Œ 'pip install json-repair' å®‰è£…")
        
        query = parameters.get("query", "")
        request_id = parameters.get("request_id", "")
        max_loop = parameters.get("max_loop", 1)
        stream = parameters.get("stream", False)
        
        search_instance = self._get_search_instance()
        
        results = []
        final_answer = ""
        all_docs = []
        
        async for chunk in search_instance.run(
            query=query,
            request_id=request_id,
            max_loop=max_loop,
            stream=stream
        ):
            if stream:
                results.append(chunk)
            else:
                results.append(chunk)
            
            # å°è¯•è§£æ JSON å¹¶æå–ç­”æ¡ˆ
            try:
                chunk_data = json.loads(chunk)
                if chunk_data.get("messageType") == "report" and chunk_data.get("answer"):
                    final_answer += chunk_data.get("answer", "")
                # æ”¶é›†æœç´¢ç»“æœ
                search_result = chunk_data.get("searchResult", {})
                if search_result.get("docs"):
                    for docs_list in search_result.get("docs", []):
                        all_docs.extend(docs_list)
            except (json.JSONDecodeError, TypeError):
                # å¦‚æœä¸æ˜¯ JSONï¼Œå¯èƒ½æ˜¯çº¯æ–‡æœ¬ç­”æ¡ˆ
                if chunk and not chunk.startswith("{"):
                    final_answer += chunk
        
        # å¦‚æœ stream=Falseï¼Œè§£ææ‰€æœ‰ç»“æœ
        if not stream:
            combined_result = "".join(results)
            try:
                # å°è¯•è§£ææœ€åä¸€ä¸ª JSONï¼ˆé€šå¸¸æ˜¯æœ€ç»ˆç­”æ¡ˆï¼‰
                lines = combined_result.strip().split("\n")
                for line in reversed(lines):
                    if line.strip().startswith("{"):
                        try:
                            data = json.loads(line)
                            if data.get("messageType") == "report" and data.get("answer"):
                                final_answer = data.get("answer", "")
                                break
                        except json.JSONDecodeError:
                            continue
            except Exception:
                pass
        
        # å¦‚æœæœ‰æœ€ç»ˆç­”æ¡ˆï¼Œè¿”å›ç­”æ¡ˆï¼›å¦åˆ™è¿”å›æ ¼å¼åŒ–çš„æœç´¢ç»“æœ
        if final_answer:
            return final_answer
        elif all_docs:
            # æ ¼å¼åŒ–æœç´¢ç»“æœ
            formatted_results = [f"å…³äº '{query}' çš„æœç´¢ç»“æœï¼š\n"]
            for i, doc in enumerate(all_docs[:10], 1):
                if isinstance(doc, dict):
                    title = doc.get("title", "æ— æ ‡é¢˜") or "æ— æ ‡é¢˜"
                    content = doc.get("content", "") or ""
                    link = doc.get("link", "") or ""
                else:
                    title = getattr(doc, 'title', 'æ— æ ‡é¢˜') or 'æ— æ ‡é¢˜'
                    content = getattr(doc, 'content', '') or ''
                    link = getattr(doc, 'link', '') or ''
                
                formatted_results.append(f"{i}. {title}")
                if content:
                    content_preview = content[:200] + "..." if len(content) > 200 else content
                    formatted_results.append(f"   å†…å®¹: {content_preview}")
                if link:
                    formatted_results.append(f"   é“¾æ¥: {link}")
                formatted_results.append("")
            
            formatted_results.append(f"å…±æ‰¾åˆ° {len(all_docs)} ä¸ªç›¸å…³ç»“æœ")
            return "\n".join(formatted_results)
        else:
            # å¦‚æœæ²¡æœ‰ç»“æœï¼Œè¿”å›åŸå§‹ JSONï¼ˆç”¨äºè°ƒè¯•ï¼‰
            if stream:
                return results
            else:
                return combined_result if results else f"æœªæ‰¾åˆ°å…³äº '{query}' çš„æœç´¢ç»“æœ"
    
    def get_parameters_schema(self) -> Dict[str, Any]:
        """è·å–å‚æ•°æ¨¡å¼"""
        return {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "æœç´¢æŸ¥è¯¢"
                },
                "request_id": {
                    "type": "string",
                    "description": "è¯·æ±‚ID",
                    "default": ""
                },
                "max_loop": {
                    "type": "integer",
                    "description": "æœ€å¤§æœç´¢è½®æ•°",
                    "default": 1
                },
                "stream": {
                    "type": "boolean",
                    "description": "æ˜¯å¦æµå¼è¿”å›",
                    "default": False
                }
            },
            "required": ["query"]
        }


class ReportTool(BaseTool):
    """æŠ¥å‘Šç”Ÿæˆå·¥å…·"""
    
    def __init__(self):
        super().__init__(
            name="report",
            description="æŠ¥å‘Šç”Ÿæˆå·¥å…·ï¼Œå¯ä»¥ç”Ÿæˆmarkdownã€htmlæˆ–pptæ ¼å¼çš„æŠ¥å‘Š",
            container_type=BaseTool.CONTAINER_TYPE_FILE,  # ç»‘å®šæ–‡ä»¶å®¹å™¨
            container_config={
                "workspace_dir": "reports",
                "allowed_formats": ["markdown", "html", "ppt"]
            }
        )
    
    async def execute(self, parameters: Dict[str, Any]) -> Any:
        """æ‰§è¡ŒæŠ¥å‘Šç”Ÿæˆ"""
        if not REPORT_AVAILABLE:
            raise RuntimeError("æŠ¥å‘Šç”Ÿæˆå·¥å…·ä¸å¯ç”¨")
        
        task = parameters.get("task", "")
        file_names = parameters.get("file_names", [])
        model = parameters.get("model", "gpt-4.1")
        file_type = parameters.get("file_type", "markdown")
        output_path = parameters.get("output_path")  # å¯é€‰ï¼šæŒ‡å®šè¾“å‡ºè·¯å¾„
        
        results = []
        async for chunk in report(
            task=task,
            file_names=file_names,
            model=model,
            file_type=file_type
        ):
            results.append(chunk)
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        report_content = "".join(results)
        
        # ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
        if report_content:
            try:
                import os
                from datetime import datetime
                
                # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºè·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
                if not output_path:
                    # ä½¿ç”¨é…ç½®çš„å·¥ä½œç›®å½•
                    workspace_dir = self.container_config.get("workspace_dir", "reports")
                    os.makedirs(workspace_dir, exist_ok=True)
                    
                    # ç”Ÿæˆæ–‡ä»¶åï¼šåŸºäºä»»åŠ¡å’Œæ—¶é—´æˆ³
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    # ä»ä»»åŠ¡ä¸­æå–å…³é”®è¯ä½œä¸ºæ–‡ä»¶åï¼ˆå–å‰20ä¸ªå­—ç¬¦ï¼‰
                    task_slug = "".join(c for c in task[:20] if c.isalnum() or c in (' ', '-', '_')).strip().replace(' ', '_')
                    if not task_slug:
                        task_slug = "report"
                    
                    # æ ¹æ®æ–‡ä»¶ç±»å‹ç¡®å®šæ‰©å±•å
                    ext_map = {
                        "markdown": ".md",
                        "html": ".html",
                        "ppt": ".pptx"
                    }
                    ext = ext_map.get(file_type, ".md")
                    
                    output_path = os.path.join(workspace_dir, f"{task_slug}_{timestamp}{ext}")
                
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                output_dir = os.path.dirname(output_path)
                if output_dir:  # åªæœ‰å½“ç›®å½•ä¸ä¸ºç©ºæ—¶æ‰åˆ›å»º
                    os.makedirs(output_dir, exist_ok=True)
                
                # å†™å…¥æ–‡ä»¶ï¼ˆæ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©å†™å…¥æ¨¡å¼ï¼‰
                if file_type == "ppt":
                    # PPT æ–‡ä»¶å¯èƒ½æ˜¯äºŒè¿›åˆ¶ï¼Œä½†è¿™é‡Œå…ˆæŒ‰æ–‡æœ¬å¤„ç†
                    # å¦‚æœåç»­éœ€è¦æ”¯æŒçœŸæ­£çš„ PPT äºŒè¿›åˆ¶æ ¼å¼ï¼Œéœ€è¦é¢å¤–å¤„ç†
                    with open(output_path, 'w', encoding='utf-8') as f:
                        f.write(report_content)
                else:
                    # Markdown å’Œ HTML æ–‡ä»¶ä½¿ç”¨æ–‡æœ¬æ¨¡å¼
                    with open(output_path, 'w', encoding='utf-8') as f:
                        f.write(report_content)
                
                logger.info(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
                
                # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰
                import os
                project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
                try:
                    rel_path = os.path.relpath(output_path, project_root)
                    # ç»Ÿä¸€ä½¿ç”¨æ­£æ–œæ ä½œä¸ºè·¯å¾„åˆ†éš”ç¬¦ï¼ˆç”¨äºURLï¼‰
                    rel_path = rel_path.replace("\\", "/")
                except ValueError:
                    # å¦‚æœæ— æ³•è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä½¿ç”¨ç»å¯¹è·¯å¾„
                    rel_path = output_path.replace("\\", "/")
                
                # ç”Ÿæˆä¸‹è½½URL
                download_url = f"/api/files/download/{rel_path}"
                
                # è¿”å›ç»“æ„åŒ–çš„ç»“æœï¼ŒåŒ…å«ä¸‹è½½é“¾æ¥ä¿¡æ¯ï¼ˆJSONæ ¼å¼ï¼Œæ–¹ä¾¿å‰ç«¯è§£æï¼‰
                import json
                result_data = {
                    "message": "æŠ¥å‘Šå·²ç”Ÿæˆå¹¶ä¿å­˜",
                    "file_path": output_path,
                    "file_name": os.path.basename(output_path),
                    "download_url": download_url,
                    "file_size": len(report_content),
                    "preview": report_content[:500] + "..." if len(report_content) > 500 else report_content,
                    "full_content": report_content  # ä¿ç•™å®Œæ•´å†…å®¹ä¾›åç»­ä½¿ç”¨
                }
                
                # è¿”å›æ ¼å¼åŒ–çš„æ–‡æœ¬ï¼ŒåŒ…å«å¯ç‚¹å‡»çš„ä¸‹è½½é“¾æ¥æ ‡è®°
                return f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆå¹¶ä¿å­˜\n\nğŸ“„ æ–‡ä»¶å: {result_data['file_name']}\nğŸ“ è·¯å¾„: {output_path}\nğŸ’¾ å¤§å°: {len(report_content)} å­—ç¬¦\n\nğŸ”— [ä¸‹è½½é“¾æ¥]({download_url})\n\nğŸ“ å†…å®¹é¢„è§ˆ:\n{result_data['preview']}\n\n<!-- REPORT_DOWNLOAD_INFO: {json.dumps(result_data, ensure_ascii=False)} -->"
            except Exception as e:
                logger.error(f"ä¿å­˜æŠ¥å‘Šæ–‡ä»¶å¤±è´¥: {e}")
                # å³ä½¿ä¿å­˜å¤±è´¥ï¼Œä¹Ÿè¿”å›æŠ¥å‘Šå†…å®¹
                return f"æŠ¥å‘Šç”ŸæˆæˆåŠŸï¼Œä½†ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}\n\næŠ¥å‘Šå†…å®¹:\n{report_content}"
        
        return report_content
    
    def get_parameters_schema(self) -> Dict[str, Any]:
        """è·å–å‚æ•°æ¨¡å¼"""
        return {
            "type": "object",
            "properties": {
                "task": {
                    "type": "string",
                    "description": "æŠ¥å‘Šç”Ÿæˆä»»åŠ¡æè¿°"
                },
                "file_names": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "è¦å¤„ç†çš„æ–‡ä»¶ååˆ—è¡¨",
                    "default": []
                },
                "model": {
                    "type": "string",
                    "description": "ä½¿ç”¨çš„æ¨¡å‹",
                    "default": "gpt-4.1"
                },
                "file_type": {
                    "type": "string",
                    "enum": ["markdown", "html", "ppt"],
                    "description": "æŠ¥å‘Šç±»å‹",
                    "default": "markdown"
                },
                "output_path": {
                    "type": "string",
                    "description": "è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰",
                    "default": None
                }
            },
            "required": ["task"]
        }


# å†…ç½®å·¥å…·åˆ—è¡¨
BUILTIN_TOOLS = [
    CodeInterpreterTool,
    DeepSearchTool,
    ReportTool,
]

def get_builtin_tools() -> List[BaseTool]:
    """è·å–æ‰€æœ‰å¯ç”¨çš„å†…ç½®å·¥å…·å®ä¾‹"""
    tools = []
    for tool_class in BUILTIN_TOOLS:
        try:
            tool = tool_class()
            tools.append(tool)
        except Exception as e:
            logger.warning(f"æ— æ³•åˆ›å»ºå†…ç½®å·¥å…· {tool_class.__name__}: {e}")
    return tools

