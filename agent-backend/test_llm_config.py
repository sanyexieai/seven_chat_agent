#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLMé…ç½®æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯ä¸åŒLLMæä¾›å•†çš„é…ç½®æ˜¯å¦æ­£ç¡®
"""

import asyncio
import os
import sys
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from utils.llm_helper import get_llm_helper
from utils.log_helper import get_logger

# è·å–loggerå®ä¾‹
logger = get_logger("test_llm_config")

async def test_llm_config():
    """æµ‹è¯•LLMé…ç½®"""
    print("ğŸ§ª LLMé…ç½®æµ‹è¯•")
    print("=" * 50)
    
    # è·å–é…ç½®ä¿¡æ¯
    provider = os.getenv("MODEL_PROVIDER", "openai")
    model = os.getenv("MODEL", "gpt-3.5-turbo")
    api_key = os.getenv("API_KEY", "")
    base_url = os.getenv("BASE_URL", "https://api.openai.com/v1")
    ollama_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    
    print(f"ğŸ“‹ å½“å‰é…ç½®:")
    print(f"   æä¾›å•†: {provider}")
    print(f"   æ¨¡å‹: {model}")
    print(f"   APIå¯†é’¥: {'å·²è®¾ç½®' if api_key else 'æœªè®¾ç½®'}")
    print(f"   åŸºç¡€URL: {base_url}")
    if provider == "ollama":
        print(f"   Ollama URL: {ollama_url}")
    print()
    
    # æµ‹è¯•LLMåˆå§‹åŒ–
    try:
        print("ğŸ”§ åˆå§‹åŒ–LLMå®¢æˆ·ç«¯...")
        llm_helper = get_llm_helper()
        print("âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•ç®€å•è°ƒç”¨
        print("\nğŸ“ æµ‹è¯•ç®€å•è°ƒç”¨...")
        test_message = "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"
        response = await llm_helper.call(test_message)
        print(f"âœ… è°ƒç”¨æˆåŠŸ")
        print(f"ğŸ“¤ è¾“å…¥: {test_message}")
        print(f"ğŸ“¥ è¾“å‡º: {response[:200]}...")
        
        # æµ‹è¯•æµå¼è°ƒç”¨
        print("\nğŸŒŠ æµ‹è¯•æµå¼è°ƒç”¨...")
        print("ğŸ“¤ è¾“å…¥: è¯·ç”¨ä¸€å¥è¯æ€»ç»“äººå·¥æ™ºèƒ½")
        print("ğŸ“¥ è¾“å‡º: ", end="", flush=True)
        
        async for chunk in llm_helper.call_stream("è¯·ç”¨ä¸€å¥è¯æ€»ç»“äººå·¥æ™ºèƒ½"):
            print(chunk, end="", flush=True)
        print("\nâœ… æµå¼è°ƒç”¨æˆåŠŸ")
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼LLMé…ç½®æ­£ç¡®ã€‚")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        print("\nğŸ”§ æ•…éšœæ’é™¤å»ºè®®:")
        
        if provider == "openai":
            print("1. æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®è®¾ç½®")
            print("2. ç¡®è®¤APIå¯†é’¥æœ‰è¶³å¤Ÿçš„é…é¢")
            print("3. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
            print("4. éªŒè¯æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®")
            
        elif provider == "anthropic":
            print("1. æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®è®¾ç½®")
            print("2. ç¡®è®¤APIå¯†é’¥æœ‰è¶³å¤Ÿçš„é…é¢")
            print("3. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
            print("4. éªŒè¯æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®")
            
        elif provider == "ollama":
            print("1. ç¡®ä¿OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ: ollama serve")
            print("2. æ£€æŸ¥ç«¯å£11434æ˜¯å¦å¯è®¿é—®")
            print("3. éªŒè¯æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½: ollama list")
            print("4. æµ‹è¯•æ¨¡å‹: ollama run llama2 'Hello'")
            
        else:
            print("1. æ£€æŸ¥MODEL_PROVIDERç¯å¢ƒå˜é‡")
            print("2. ç¡®ä¿æ”¯æŒè¯¥æä¾›å•†")
            
        return False
    
    return True

async def test_specific_provider(provider: str):
    """æµ‹è¯•ç‰¹å®šæä¾›å•†"""
    print(f"\nğŸ§ª æµ‹è¯• {provider} é…ç½®")
    print("-" * 30)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ["MODEL_PROVIDER"] = provider
    
    if provider == "ollama":
        os.environ["MODEL"] = "llama2"
        os.environ["OLLAMA_BASE_URL"] = "http://localhost:11434"
    elif provider == "openai":
        os.environ["MODEL"] = "gpt-3.5-turbo"
        os.environ["BASE_URL"] = "https://api.openai.com/v1"
    elif provider == "anthropic":
        os.environ["MODEL"] = "claude-3-sonnet-20240229"
    
    try:
        llm_helper = get_llm_helper()
        response = await llm_helper.call("ä½ å¥½")
        print(f"âœ… {provider} é…ç½®æ­£ç¡®")
        print(f"   å“åº”: {response[:100]}...")
        return True
    except Exception as e:
        print(f"âŒ {provider} é…ç½®é”™è¯¯: {str(e)}")
        return False

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ LLMé…ç½®æµ‹è¯•å·¥å…·")
    print("=" * 50)
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        provider = sys.argv[1]
        if provider in ["openai", "anthropic", "ollama"]:
            await test_specific_provider(provider)
        else:
            print(f"âŒ ä¸æ”¯æŒçš„æä¾›å•†: {provider}")
            print("æ”¯æŒçš„æä¾›å•†: openai, anthropic, ollama")
    else:
        # æµ‹è¯•å½“å‰é…ç½®
        success = await test_llm_config()
        
        if not success:
            print("\nğŸ’¡ æç¤º:")
            print("1. æ£€æŸ¥ .env æ–‡ä»¶é…ç½®")
            print("2. ç¡®ä¿ç¯å¢ƒå˜é‡æ­£ç¡®è®¾ç½®")
            print("3. å‚è€ƒ LLM_SETUP.md æ–‡æ¡£")
            print("4. è¿è¡Œ python test_llm_config.py <provider> æµ‹è¯•ç‰¹å®šæä¾›å•†")

if __name__ == "__main__":
    asyncio.run(main()) 