#!/usr/bin/env python3
"""
LLMé…ç½®åˆå§‹åŒ–è„šæœ¬
"""

import os
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from database.database import SessionLocal
from services.llm_config_service import LLMConfigService
from models.database_models import LLMConfigCreate
from utils.log_helper import get_logger

logger = get_logger("init_llm_configs")

def init_default_llm_configs():
    """åˆå§‹åŒ–é»˜è®¤LLMé…ç½®"""
    db = SessionLocal()
    try:
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰é…ç½®
        existing_configs = LLMConfigService.get_all_configs(db)
        if existing_configs:
            logger.info(f"å·²å­˜åœ¨ {len(existing_configs)} ä¸ªLLMé…ç½®ï¼Œè·³è¿‡åˆå§‹åŒ–")
            return
        
        # é»˜è®¤é…ç½®åˆ—è¡¨
        default_configs = [
            {
                "name": "openai_gpt4",
                "display_name": "OpenAI GPT-4",
                "description": "OpenAI GPT-4 æ¨¡å‹é…ç½®",
                "provider": "openai",
                "model_name": "gpt-4",
                "api_key": os.getenv("OPENAI_API_KEY", ""),
                "api_base": "https://api.openai.com/v1",
                "config": {
                    "temperature": 0.7,
                    "max_tokens": 2048
                },
                "is_default": True
            },
            {
                "name": "openai_gpt35",
                "display_name": "OpenAI GPT-3.5",
                "description": "OpenAI GPT-3.5 Turbo æ¨¡å‹é…ç½®",
                "provider": "openai",
                "model_name": "gpt-3.5-turbo",
                "api_key": os.getenv("OPENAI_API_KEY", ""),
                "api_base": "https://api.openai.com/v1",
                "config": {
                    "temperature": 0.7,
                    "max_tokens": 2048
                },
                "is_default": False
            },
            {
                "name": "anthropic_claude",
                "display_name": "Anthropic Claude",
                "description": "Anthropic Claude æ¨¡å‹é…ç½®",
                "provider": "anthropic",
                "model_name": "claude-3-sonnet-20240229",
                "api_key": os.getenv("ANTHROPIC_API_KEY", ""),
                "api_base": "https://api.anthropic.com",
                "config": {
                    "temperature": 0.7,
                    "max_tokens": 2048
                },
                "is_default": False
            },
            {
                "name": "ollama_qwen",
                "display_name": "Ollama Qwen æ¨¡å‹",
                "description": "Ollama Qwen æ¨¡å‹é…ç½®",
                "provider": "ollama",
                "model_name": "qwen2.5:7b",
                "api_key": "",
                "api_base": "http://localhost:11434",
                "config": {
                    "temperature": 0.7,
                    "max_tokens": 2048
                },
                "is_default": False
            },
            {
                "name": "ollama_llama",
                "display_name": "Ollama Llama æ¨¡å‹",
                "description": "Ollama Llama æ¨¡å‹é…ç½®",
                "provider": "ollama",
                "model_name": "llama3.2:3b",
                "api_key": "",
                "api_base": "http://localhost:11434",
                "config": {
                    "temperature": 0.7,
                    "max_tokens": 2048
                },
                "is_default": False
            },
            {
                "name": "ollama_mistral",
                "display_name": "Ollama Mistral æ¨¡å‹",
                "description": "Ollama Mistral æ¨¡å‹é…ç½®",
                "provider": "ollama",
                "model_name": "mistral:7b",
                "api_key": "",
                "api_base": "http://localhost:11434",
                "config": {
                    "temperature": 0.7,
                    "max_tokens": 2048
                },
                "is_default": False
            },
            {
                "name": "ollama_codellama",
                "display_name": "Ollama Code Llama æ¨¡å‹",
                "description": "Ollama Code Llama ä»£ç ç”Ÿæˆæ¨¡å‹é…ç½®",
                "provider": "ollama",
                "model_name": "codellama:7b",
                "api_key": "",
                "api_base": "http://localhost:11434",
                "config": {
                    "temperature": 0.7,
                    "max_tokens": 2048
                },
                "is_default": False
            },
            {
                "name": "ollama_neural",
                "display_name": "Ollama Neural Chat æ¨¡å‹",
                "description": "Ollama Neural Chat æ¨¡å‹é…ç½®",
                "provider": "ollama",
                "model_name": "neural-chat:7b",
                "api_key": "",
                "api_base": "http://localhost:11434",
                "config": {
                    "temperature": 0.7,
                    "max_tokens": 2048
                },
                "is_default": False
            },
            {
                "name": "deepseek_chat",
                "display_name": "DeepSeek Chat",
                "description": "DeepSeek Chat æ¨¡å‹é…ç½®",
                "provider": "deepseek",
                "model_name": "deepseek-chat",
                "api_key": os.getenv("DEEPSEEK_API_KEY", ""),
                "api_base": "https://api.deepseek.com",
                "config": {
                    "temperature": 0.7,
                    "max_tokens": 2048
                },
                "is_default": False
            },
            {
                "name": "deepseek_coder",
                "display_name": "DeepSeek Coder",
                "description": "DeepSeek Coder ä»£ç ç”Ÿæˆæ¨¡å‹é…ç½®",
                "provider": "deepseek",
                "model_name": "deepseek-coder",
                "api_key": os.getenv("DEEPSEEK_API_KEY", ""),
                "api_base": "https://api.deepseek.com",
                "config": {
                    "temperature": 0.7,
                    "max_tokens": 2048
                },
                "is_default": False
            }
        ]
        
        # åˆ›å»ºé…ç½®
        for config_data in default_configs:
            try:
                config_create = LLMConfigCreate(**config_data)
                config = LLMConfigService.create_config(db, config_create)
                logger.info(f"åˆ›å»ºLLMé…ç½®: {config.display_name}")
            except Exception as e:
                logger.error(f"åˆ›å»ºLLMé…ç½®å¤±è´¥ {config_data['name']}: {str(e)}")
        
        logger.info("LLMé…ç½®åˆå§‹åŒ–å®Œæˆ")
        
    except Exception as e:
        logger.error(f"LLMé…ç½®åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        raise
    finally:
        db.close()

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹åˆå§‹åŒ–LLMé…ç½®...")
    init_default_llm_configs()
    print("âœ… LLMé…ç½®åˆå§‹åŒ–å®Œæˆ") 