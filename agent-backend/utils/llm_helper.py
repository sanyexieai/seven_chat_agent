# -*- coding: utf-8 -*-
import os
import asyncio
from typing import Optional, Dict, Any, AsyncGenerator, List
import json
import httpx
import openai
import anthropic
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic

from config.env import MODEL, MODEL_PROVIDER, TEMPERATURE, BASE_URL, API_KEY
from utils.log_helper import get_logger

# è·å–loggerå®ä¾‹
logger = get_logger("llm_helper")

class LLMHelper:
    """å¤§æ¨¡å‹åŠ©æ‰‹å•ä¾‹ç±»ï¼Œæ”¯æŒå¤šæ¨¡å‹é…ç½®ã€åŠ¨æ€åˆ‡æ¢ã€ä¾¿æ·è°ƒç”¨ã€‚"""
    _instance: Optional['LLMHelper'] = None
    _initialized: bool = False

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if not self._initialized:
            self._config = {}
            self._openai_client = None
            self._anthropic_client = None
            self._ollama_base_url = None
            self._initialized = True

    def setup(self, **kwargs) -> 'LLMHelper':
        """
        é…ç½®å¹¶åˆå§‹åŒ–å¤§æ¨¡å‹
        """
        self._config = dict(
            model=MODEL, 
            model_provider=MODEL_PROVIDER, 
            temperature=TEMPERATURE, 
            base_url=BASE_URL, 
            api_key=API_KEY
        )
        self._config.update(kwargs)
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self._init_clients()
        
        return self

    def _init_clients(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        try:
            provider = self._config.get('model_provider')
            
            if provider == 'openai':
                self._openai_client = AsyncOpenAI(
                    api_key=self._config.get('api_key'),
                    base_url=self._config.get('base_url')
                )
                logger.info("OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            elif provider == 'anthropic':
                self._anthropic_client = AsyncAnthropic(
                    api_key=self._config.get('api_key')
                )
                logger.info("Anthropicå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            elif provider == 'ollama':
                self._ollama_base_url = self._config.get('base_url', 'http://localhost:11434')
                logger.info(f"Ollamaå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ŒåŸºç¡€URL: {self._ollama_base_url}")
            else:
                raise Exception(f"ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†: {provider}")
                
        except Exception as e:
            logger.error(f"LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise

    async def call(self, messages, **kwargs):
        """
        å¼‚æ­¥è°ƒç”¨å¤§æ¨¡å‹
        :param messages: èŠå¤©æ¶ˆæ¯ï¼ˆlist/dict/strï¼‰
        :param kwargs: å…¶ä»–å‚æ•°
        :return: å¤§æ¨¡å‹è¾“å‡º
        """
        try:
            # å¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥
            if isinstance(messages, str):
                # å­—ç¬¦ä¸²è¾“å…¥ï¼Œè½¬æ¢ä¸ºæ¶ˆæ¯æ ¼å¼
                messages = [{"role": "user", "content": messages}]
            elif isinstance(messages, list):
                # ç¡®ä¿æ¶ˆæ¯æ ¼å¼æ­£ç¡®
                formatted_messages = []
                for msg in messages:
                    if isinstance(msg, dict):
                        formatted_messages.append(msg)
                    elif isinstance(msg, str):
                        formatted_messages.append({"role": "user", "content": msg})
                    else:
                        formatted_messages.append({"role": "user", "content": str(msg)})
                messages = formatted_messages
            
            # è°ƒç”¨å¯¹åº”çš„LLM
            provider = self._config.get('model_provider')
            if provider == 'openai':
                return await self._call_openai(messages, **kwargs)
            elif provider == 'anthropic':
                return await self._call_anthropic(messages, **kwargs)
            elif provider == 'ollama':
                return await self._call_ollama(messages, **kwargs)
            else:
                raise Exception(f"ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†: {provider}")
                
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    async def _call_openai(self, messages: List[Dict], **kwargs) -> str:
        """è°ƒç”¨OpenAI API"""
        try:
            if not self._openai_client:
                raise Exception("OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
            # åˆå¹¶é…ç½®å‚æ•°
            params = {
                "model": self._config.get('model', 'gpt-3.5-turbo'),
                "messages": messages,
                "temperature": self._config.get('temperature', 0.7),
                "max_tokens": kwargs.get('max_tokens', 1000),
                **kwargs
            }
            
            response = await self._openai_client.chat.completions.create(**params)
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"OpenAIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    async def _call_anthropic(self, messages: List[Dict], **kwargs) -> str:
        """è°ƒç”¨Anthropic API"""
        try:
            if not self._anthropic_client:
                raise Exception("Anthropicå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            system_message = ""
            user_messages = []
            
            for msg in messages:
                if msg.get('role') == 'system':
                    system_message = msg.get('content', '')
                elif msg.get('role') == 'user':
                    user_messages.append(msg.get('content', ''))
                elif msg.get('role') == 'assistant':
                    # Anthropicä¸æ”¯æŒassistantæ¶ˆæ¯ï¼Œè·³è¿‡
                    continue
            
            # åˆå¹¶ç”¨æˆ·æ¶ˆæ¯
            user_content = "\n".join(user_messages)
            
            # è°ƒç”¨API
            params = {
                "model": self._config.get('model', 'claude-3-sonnet-20240229'),
                "max_tokens": kwargs.get('max_tokens', 1000),
                "temperature": self._config.get('temperature', 0.7),
                **kwargs
            }
            
            if system_message:
                params["system"] = system_message
            
            response = await self._anthropic_client.messages.create(
                messages=[{"role": "user", "content": user_content}],
                **params
            )
            
            return response.content[0].text
            
        except Exception as e:
            logger.error(f"Anthropicè°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    async def _call_ollama(self, messages: List[Dict], **kwargs) -> str:
        """è°ƒç”¨Ollama API"""
        try:
            if not self._ollama_base_url:
                raise Exception("Ollamaå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
            # æ„å»ºè¯·æ±‚æ•°æ®
            data = {
                "model": self._config.get('model', 'llama2'),
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": self._config.get('temperature', 0.7),
                    "num_predict": kwargs.get('max_tokens', 1000),
                    **kwargs.get('options', {})
                }
            }
            
            # å‘é€è¯·æ±‚
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{self._ollama_base_url}/api/chat",
                    json=data,
                    timeout=60
                )
                response.raise_for_status()
                
                result = response.json()
                return result.get('message', {}).get('content', '')
                
        except Exception as e:
            logger.error(f"Ollamaè°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    async def call_stream(self, messages, **kwargs) -> AsyncGenerator[str, None]:
        """
        æµå¼è°ƒç”¨å¤§æ¨¡å‹
        :param messages: èŠå¤©æ¶ˆæ¯
        :param kwargs: å…¶ä»–å‚æ•°
        :yield: æµå¼å“åº”ç‰‡æ®µ
        """
        try:
            # å¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥
            if isinstance(messages, str):
                messages = [{"role": "user", "content": messages}]
            elif isinstance(messages, list):
                formatted_messages = []
                for msg in messages:
                    if isinstance(msg, dict):
                        formatted_messages.append(msg)
                    elif isinstance(msg, str):
                        formatted_messages.append({"role": "user", "content": msg})
                    else:
                        formatted_messages.append({"role": "user", "content": str(msg)})
                messages = formatted_messages
            
            # è°ƒç”¨å¯¹åº”çš„æµå¼LLM
            provider = self._config.get('model_provider')
            if provider == 'openai':
                async for chunk in self._call_openai_stream(messages, **kwargs):
                    yield chunk
            elif provider == 'anthropic':
                async for chunk in self._call_anthropic_stream(messages, **kwargs):
                    yield chunk
            elif provider == 'ollama':
                async for chunk in self._call_ollama_stream(messages, **kwargs):
                    yield chunk
            else:
                raise Exception(f"ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†: {provider}")
                    
        except Exception as e:
            logger.error(f"LLMæµå¼è°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    async def _call_openai_stream(self, messages: List[Dict], **kwargs) -> AsyncGenerator[str, None]:
        """æµå¼è°ƒç”¨OpenAI API"""
        try:
            if not self._openai_client:
                raise Exception("OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
            params = {
                "model": self._config.get('model', 'gpt-3.5-turbo'),
                "messages": messages,
                "temperature": self._config.get('temperature', 0.7),
                "max_tokens": kwargs.get('max_tokens', 1000),
                "stream": True,
                **kwargs
            }
            
            stream = await self._openai_client.chat.completions.create(**params)
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            logger.error(f"OpenAIæµå¼è°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    async def _call_anthropic_stream(self, messages: List[Dict], **kwargs) -> AsyncGenerator[str, None]:
        """æµå¼è°ƒç”¨Anthropic API"""
        try:
            if not self._anthropic_client:
                raise Exception("Anthropicå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            system_message = ""
            user_messages = []
            
            for msg in messages:
                if msg.get('role') == 'system':
                    system_message = msg.get('content', '')
                elif msg.get('role') == 'user':
                    user_messages.append(msg.get('content', ''))
                elif msg.get('role') == 'assistant':
                    continue
            
            user_content = "\n".join(user_messages)
            
            params = {
                "model": self._config.get('model', 'claude-3-sonnet-20240229'),
                "max_tokens": kwargs.get('max_tokens', 1000),
                "temperature": self._config.get('temperature', 0.7),
                **kwargs
            }
            
            if system_message:
                params["system"] = system_message
            
            stream = await self._anthropic_client.messages.create(
                messages=[{"role": "user", "content": user_content}],
                stream=True,
                **params
            )
            
            async for chunk in stream:
                if chunk.type == "content_block_delta":
                    yield chunk.delta.text
                    
        except Exception as e:
            logger.error(f"Anthropicæµå¼è°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    async def _call_ollama_stream(self, messages: List[Dict], **kwargs) -> AsyncGenerator[str, None]:
        """æµå¼è°ƒç”¨Ollama API"""
        try:
            if not self._ollama_base_url:
                raise Exception("Ollamaå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
            # æ„å»ºè¯·æ±‚æ•°æ®
            data = {
                "model": self._config.get('model', 'llama2'),
                "messages": messages,
                "stream": True,
                "options": {
                    "temperature": self._config.get('temperature', 0.7),
                    "num_predict": kwargs.get('max_tokens', 1000),
                    **kwargs.get('options', {})
                }
            }
            
            # å‘é€æµå¼è¯·æ±‚
            async with httpx.AsyncClient() as client:
                async with client.stream(
                    "POST",
                    f"{self._ollama_base_url}/api/chat",
                    json=data,
                    timeout=60
                ) as response:
                    response.raise_for_status()
                    
                    async for line in response.aiter_lines():
                        if line.strip():
                            try:
                                chunk_data = json.loads(line)
                                if 'message' in chunk_data and 'content' in chunk_data['message']:
                                    yield chunk_data['message']['content']
                            except json.JSONDecodeError:
                                continue
                                
        except Exception as e:
            logger.error(f"Ollamaæµå¼è°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    def switch_model(self, model: str, **kwargs):
        """
        åŠ¨æ€åˆ‡æ¢æ¨¡å‹
        """
        cfg = self._config.copy()
        cfg.update(model=model)
        cfg.update(kwargs)
        self._config = cfg
        self._init_clients()
        return self

    def get_config(self) -> dict:
        """è·å–å½“å‰æ¨¡å‹é…ç½®"""
        return self._config.copy()

# è·å–å•ä¾‹
def get_llm_helper():
    llm_helper = LLMHelper()
    llm_helper.setup()
    return llm_helper

def get_llm(model: str = None, **kwargs):
    """
    è·å–å¤§æ¨¡å‹å®ä¾‹çš„ä¾¿æ·å‡½æ•°
    :param model: æŒ‡å®šæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
    :param kwargs: å…¶ä»–å‚æ•°
    :return: å¤§æ¨¡å‹å®ä¾‹
    """
    helper = get_llm_helper()
    if model:
        return helper.switch_model(model, **kwargs)
    return helper

def setup_llm(**kwargs) -> LLMHelper:
    """
    é…ç½®å¹¶åˆå§‹åŒ–å¤§æ¨¡å‹çš„ä¾¿æ·å‡½æ•°
    :param kwargs: é…ç½®å‚æ•°
    :return: LLMHelperå®ä¾‹
    """
    #å°è¯•ä»ç¯å¢ƒå˜é‡è·å–å‚æ•°
    if "MODEL_PROVIDER" in os.environ:
        kwargs["model_provider"] = os.environ["MODEL_PROVIDER"]
    if "MODEL" in os.environ:
        kwargs["model"] = os.environ["MODEL"]
    if "TEMPERATURE" in os.environ:
        kwargs["temperature"] = float(os.environ["TEMPERATURE"])
    
    llm_helper = LLMHelper()
    return llm_helper.setup(**kwargs)

if __name__ == "__main__":
    async def test_llm_helper():
        """æµ‹è¯•LLMåŠ©æ‰‹"""
        print("ğŸ§ª æµ‹è¯•LLMåŠ©æ‰‹...")
        
        # åˆ›å»ºLLMåŠ©æ‰‹
        llm_helper = get_llm_helper()
        
        # æµ‹è¯•è°ƒç”¨
        test_prompts = [
            "ä½ å¥½",
            "è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
            "ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ",
            "æœç´¢äººå·¥æ™ºèƒ½ä¿¡æ¯"
        ]
        
        for prompt in test_prompts:
            print(f"\nğŸ“ è¾“å…¥: {prompt}")
            try:
                response = await llm_helper.call(prompt)
                print(f"ğŸ¤– è¾“å‡º: {response}")
            except Exception as e:
                print(f"âŒ é”™è¯¯: {str(e)}")
        
        print("\nâœ… LLMåŠ©æ‰‹æµ‹è¯•å®Œæˆ")

    asyncio.run(test_llm_helper())