"""è§„åˆ’èŠ‚ç‚¹å®ç°ï¼šæ ¹æ®ä»»åŠ¡è‡ªåŠ¨ç”Ÿæˆæµç¨‹å›¾å¹¶æ‰§è¡Œ"""
from typing import Dict, Any, AsyncGenerator, Optional, List, Tuple
from collections import defaultdict
import json
import re

from models.chat_models import AgentMessage, StreamChunk
from utils.log_helper import get_logger
from utils.llm_helper import get_llm_helper
from ..base_node import BaseFlowNode
from ..engine import FlowEngine

logger = get_logger("flow_planner_node")

# è§„åˆ’èŠ‚ç‚¹çš„é»˜è®¤æç¤ºè¯
PLANNER_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæµç¨‹å›¾è§„åˆ’ä¸“å®¶ã€‚æ ¹æ®ç”¨æˆ·ä»»åŠ¡ï¼Œç”Ÿæˆä¸€ä¸ªå¯æ‰§è¡Œçš„æµç¨‹å›¾é…ç½®ã€‚

æµç¨‹å›¾é…ç½®æ ¼å¼ï¼ˆJSONï¼‰ï¼š
{
  "nodes": [
    {
      "id": "èŠ‚ç‚¹å”¯ä¸€ID",
      "type": "èŠ‚ç‚¹ç±»å‹ï¼ˆstart/end/llm/tool/router/auto_inferç­‰ï¼‰",
      "category": "èŠ‚ç‚¹ç±»åˆ«ï¼ˆstart/end/processor/routerï¼‰",
      "implementation": "èŠ‚ç‚¹å®ç°ï¼ˆstart/end/llm/tool/router_llm/auto_inferç­‰ï¼‰",
      "position": {"x": æ•°å­—, "y": æ•°å­—},
      "data": {
        "label": "èŠ‚ç‚¹æ˜¾ç¤ºåç§°",
        "nodeType": "èŠ‚ç‚¹ç±»å‹ï¼ˆä¸typeç›¸åŒï¼‰",
        "config": {
          // èŠ‚ç‚¹ç‰¹å®šé…ç½®
          // å¯¹äº tool èŠ‚ç‚¹ï¼štool_name, tool_type, server, params ç­‰
          // å¯¹äº llm èŠ‚ç‚¹ï¼šsystem_prompt, user_prompt ç­‰
          // å¯¹äº auto_infer èŠ‚ç‚¹ï¼štarget_tool_node_id, auto_param_key ç­‰
        },
        "isStartNode": true/false,
        "isEndNode": true/false
      }
    }
  ],
  "edges": [
    {
      "id": "è¾¹å”¯ä¸€ID",
      "source": "æºèŠ‚ç‚¹ID",
      "target": "ç›®æ ‡èŠ‚ç‚¹ID",
      "type": "default"
    }
  ],
  "metadata": {
    "name": "æµç¨‹å›¾åç§°",
    "description": "æµç¨‹å›¾æè¿°",
    "version": "1.0.0"
  }
}

å¯ç”¨èŠ‚ç‚¹ç±»å‹ï¼š
- start: å¼€å§‹èŠ‚ç‚¹ï¼ˆå¿…é¡»æœ‰ä¸”åªæœ‰ä¸€ä¸ªï¼‰
- end: ç»“æŸèŠ‚ç‚¹ï¼ˆå¿…é¡»æœ‰ä¸”åªæœ‰ä¸€ä¸ªï¼‰
- llm: LLMè°ƒç”¨èŠ‚ç‚¹
- tool: å·¥å…·è°ƒç”¨èŠ‚ç‚¹ï¼ˆéœ€è¦é…ç½® tool_name, tool_type, serverï¼‰
- auto_infer: è‡ªåŠ¨æ¨ç†èŠ‚ç‚¹ï¼ˆç”¨äºå·¥å…·å‚æ•°æ¨ç†ï¼‰
- router: è·¯ç”±èŠ‚ç‚¹ï¼ˆæ¡ä»¶åˆ¤æ–­ï¼‰

é‡è¦è§„åˆ™ï¼š
1. **ä¸è¦åŒ…å« start å’Œ end èŠ‚ç‚¹**ï¼ˆè¿™äº›èŠ‚ç‚¹ä¼šåœ¨æ‰§è¡Œæ—¶è‡ªåŠ¨æ·»åŠ ï¼‰
2. **æ‰€æœ‰èŠ‚ç‚¹å¿…é¡»ä¸²è¡Œè¿æ¥**ï¼ˆä¸€ä¸ªæ¥ä¸€ä¸ªï¼Œå½¢æˆä¸€æ¡é“¾ï¼Œä¸èƒ½æœ‰åˆ†æ”¯æˆ–å¹¶è¡Œï¼‰
3. **æ‰€æœ‰èŠ‚ç‚¹éƒ½å¿…é¡»åœ¨ä»å¼€å§‹åˆ°ç»“æŸçš„è·¯å¾„ä¸Š**ï¼ˆä¸èƒ½æœ‰æ¸¸ç¦»èŠ‚ç‚¹ï¼‰
4. å¦‚æœä½¿ç”¨ tool èŠ‚ç‚¹ï¼Œå»ºè®®åœ¨å‰é¢æ·»åŠ  auto_infer èŠ‚ç‚¹æ¥è‡ªåŠ¨ç”Ÿæˆå‚æ•°
5. èŠ‚ç‚¹ ID å¿…é¡»å”¯ä¸€
6. åªè¾“å‡º JSONï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—è¯´æ˜"""

PLANNER_USER_PROMPT_TEMPLATE = """è¯·ä¸ºä»¥ä¸‹ä»»åŠ¡ç”Ÿæˆä¸€ä¸ªæµç¨‹å›¾é…ç½®ï¼š

ä»»åŠ¡ï¼š{task}

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

{error_context}

å½“å‰è§„åˆ’ä¿¡æ¯ï¼š
- è§„åˆ’èŠ‚ç‚¹IDï¼š{planner_id}
- æœ¬æ¬¡è§„åˆ’åºå·ï¼ˆ0è¡¨ç¤ºé¦–æ¬¡è§„åˆ’ï¼Œ>=1è¡¨ç¤ºç¬¬å‡ æ¬¡é‡è¯•ï¼‰ï¼š{retry_index}

å¯ç”¨å·¥å…·åˆ—è¡¨ï¼š
{available_tools}

å·¥å…·ä½¿ç”¨è§„åˆ™ï¼š
1. **å†…ç½®å·¥å…·**ï¼štool_type ä¸º "builtin"ï¼Œtool_name ç›´æ¥ä½¿ç”¨å·¥å…·åç§°ï¼ˆå¦‚ "report", "deep_search"ï¼‰ï¼Œä¸éœ€è¦ server å‚æ•°
2. **MCPå·¥å…·**ï¼štool_type ä¸º "mcp"ï¼Œtool_name æ ¼å¼ä¸º "mcp_{{server}}_{{tool_name}}"ï¼ˆå¦‚ "mcp_1_search"ï¼‰ï¼Œserver ä¸ºæœåŠ¡å™¨ç¼–å·ï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼Œå¦‚ "1"ï¼‰
3. **ä¸´æ—¶å·¥å…·**ï¼štool_type ä¸º "temporary"ï¼Œtool_name æ ¼å¼ä¸º "temp_{{tool_name}}"ï¼Œä¸éœ€è¦ server å‚æ•°
4. ä½¿ç”¨å·¥å…·æ—¶ï¼Œ**å¿…é¡»**åœ¨å‰é¢æ·»åŠ  auto_infer èŠ‚ç‚¹æ¥è‡ªåŠ¨ç”Ÿæˆå‚æ•°
5. auto_infer èŠ‚ç‚¹çš„ target_tool_node_id åº”è¯¥æŒ‡å‘å¯¹åº”çš„ tool èŠ‚ç‚¹ ID

ID ä¸è¿çº¿è§„åˆ™ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š
1. æ‰€æœ‰èŠ‚ç‚¹ id å¿…é¡»ä½¿ç”¨æ ¼å¼ï¼š`{planner_id}_retry_{retry_index}_N`
   - å…¶ä¸­ `N` ä» 1 å¼€å§‹é€’å¢ï¼ˆ1, 2, 3, ...ï¼‰ï¼Œä¸è¦è·³å·ä¹Ÿä¸è¦å¤ç”¨æ—§çš„ N
2. é‡æ–°è§„åˆ’ï¼ˆretry_index >= 1ï¼‰æ—¶ï¼š
   - æœ¬æ¬¡ç”Ÿæˆçš„æ‰€æœ‰èŠ‚ç‚¹ id å¿…é¡»æ˜¯å…¨æ–°çš„ï¼Œ**ä¸å¾—ä¸å†å²èŠ‚ç‚¹ id ç›¸åŒ**
   - ç¦æ­¢å¤ç”¨ä¹‹å‰è§„åˆ’äº§ç”Ÿçš„ä»»ä½•èŠ‚ç‚¹ id
3. edges ä¸­çš„ source å’Œ targetï¼š
   - å¿…é¡»å…¨éƒ¨æ¥è‡ªæœ¬æ¬¡ `nodes` æ•°ç»„ä¸­å®šä¹‰çš„ id
   - **ä¸¥ç¦**è¿æ¥åˆ°å†å²èŠ‚ç‚¹æˆ–ç³»ç»Ÿè‡ªåŠ¨åˆ›å»ºçš„èŠ‚ç‚¹ï¼ˆä¾‹å¦‚å¼€å§‹ã€ç»“æŸæˆ–ä¹‹å‰è§„åˆ’äº§ç”Ÿçš„èŠ‚ç‚¹ï¼‰
4. ä¸è¦åœ¨æœ¬æ¬¡è¾“å‡ºä¸­åŒ…å«ä»»ä½• start / end èŠ‚ç‚¹ï¼Œä¹Ÿä¸è¦è¿æ¥åˆ°è¿™äº›èŠ‚ç‚¹

è¯·ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„æµç¨‹å›¾é…ç½® JSONï¼Œç¡®ä¿ï¼š
1. **ä¸è¦åŒ…å« start å’Œ end èŠ‚ç‚¹**ï¼ˆè¿™äº›èŠ‚ç‚¹ä¼šåœ¨æ‰§è¡Œæ—¶è‡ªåŠ¨æ·»åŠ ï¼‰
2. **æ‰€æœ‰èŠ‚ç‚¹å¿…é¡»ä¸²è¡Œè¿æ¥**ï¼ˆèŠ‚ç‚¹1 -> èŠ‚ç‚¹2 -> èŠ‚ç‚¹3 -> ...ï¼Œå½¢æˆä¸€æ¡é“¾ï¼Œä¸èƒ½æœ‰åˆ†æ”¯ï¼‰
3. **æ‰€æœ‰èŠ‚ç‚¹éƒ½å¿…é¡»åœ¨è·¯å¾„ä¸Š**ï¼ˆæ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰ä¸”ä»…æœ‰ä¸€ä¸ªå‰é©±å’Œä¸€ä¸ªåç»§ï¼Œé™¤äº†ç¬¬ä¸€ä¸ªèŠ‚ç‚¹æ²¡æœ‰å‰é©±ï¼Œæœ€åä¸€ä¸ªèŠ‚ç‚¹æ²¡æœ‰åç»§ï¼‰
4. èŠ‚ç‚¹é…ç½®å®Œæ•´ï¼š
   - tool èŠ‚ç‚¹ï¼šå¿…é¡»åŒ…å« tool_name, tool_type, serverï¼ˆMCPå·¥å…·éœ€è¦ï¼‰
   - auto_infer èŠ‚ç‚¹ï¼šå¿…é¡»åŒ…å« target_tool_node_idï¼ˆæŒ‡å‘å¯¹åº”çš„ tool èŠ‚ç‚¹ï¼‰
5. å¦‚æœä½¿ç”¨å·¥å…·ï¼Œ**å¿…é¡»**åœ¨å‰é¢æ·»åŠ  auto_infer èŠ‚ç‚¹
6. æµç¨‹å›¾é€»è¾‘æ¸…æ™°ï¼Œèƒ½å¤Ÿå®Œæˆä»»åŠ¡
7. ä¼˜å…ˆä½¿ç”¨ç³»ç»Ÿæä¾›çš„å·¥å…·ï¼Œæ ¹æ®ä»»åŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚çš„å·¥å…·

**é‡è¦**ï¼šedges æ•°ç»„åº”è¯¥æŒ‰ç…§èŠ‚ç‚¹é¡ºåºè¿æ¥ï¼Œä¾‹å¦‚ï¼š
- å¦‚æœæœ‰3ä¸ªèŠ‚ç‚¹ [node1, node2, node3]ï¼Œedges åº”è¯¥æ˜¯ [{{"source": "node1", "target": "node2"}}, {{"source": "node2", "target": "node3"}}]
- ä¸èƒ½æœ‰å¤šä¸ªèŠ‚ç‚¹æŒ‡å‘åŒä¸€ä¸ªèŠ‚ç‚¹ï¼Œä¹Ÿä¸èƒ½æœ‰ä¸€ä¸ªèŠ‚ç‚¹æŒ‡å‘å¤šä¸ªèŠ‚ç‚¹
- æ‰€æœ‰ edges çš„ source/target å¿…é¡»æ¥è‡ªæœ¬æ¬¡ nodes æ•°ç»„ä¸­å®šä¹‰çš„ idï¼Œ**ç¦æ­¢è¿æ¥åˆ°å†å²èŠ‚ç‚¹æˆ–ç³»ç»Ÿè‡ªåŠ¨åˆ›å»ºçš„èŠ‚ç‚¹**ï¼ˆä¾‹å¦‚å¼€å§‹ã€ç»“æŸæˆ–ä¹‹å‰è§„åˆ’äº§ç”Ÿçš„èŠ‚ç‚¹ï¼‰
- å½“ä¸Šæ–‡ä¸­åŒ…å«é”™è¯¯ä¿¡æ¯ï¼ˆè¯´æ˜è¿™æ˜¯é‡æ–°è§„åˆ’ï¼‰æ—¶ï¼šæœ¬æ¬¡ç”Ÿæˆçš„æ‰€æœ‰èŠ‚ç‚¹ id **å¿…é¡»æ˜¯å…¨æ–°çš„ï¼Œä¸å¾—ä¸å†å²èŠ‚ç‚¹ id é‡å¤**ï¼Œä¸è¦å¤ç”¨ä¹‹å‰çš„èŠ‚ç‚¹ id

åªè¾“å‡º JSON é…ç½®ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"""


class PlannerNode(BaseFlowNode):
	"""è§„åˆ’èŠ‚ç‚¹ï¼šæ ¹æ®ä»»åŠ¡è‡ªåŠ¨ç”Ÿæˆæµç¨‹å›¾å¹¶æ‰§è¡Œ"""
	
	def __init__(self, *args, **kwargs):
		super().__init__(*args, **kwargs)
		self._generated_flow_config: Optional[Dict[str, Any]] = None
		self._subflow_engine: Optional[FlowEngine] = None
	
	async def execute(self, user_id: str, message: str, context: Dict[str, Any], agent_name: str = None) -> AgentMessage:
		"""æ‰§è¡Œè§„åˆ’èŠ‚ç‚¹ï¼ˆåŒæ­¥ï¼‰"""
		try:
			# 1. ç”Ÿæˆæµç¨‹å›¾é…ç½®
			flow_config = await self._generate_flow_config(message, context)
			if not flow_config:
				error_msg = "è§„åˆ’èŠ‚ç‚¹ï¼šç”Ÿæˆæµç¨‹å›¾é…ç½®å¤±è´¥"
				logger.error(error_msg)
				return self._create_agent_message(error_msg, agent_name, metadata={'error': error_msg})
			
			# 2. åˆ›å»ºå¹¶æ‰§è¡Œæµç¨‹å›¾
			result = await self._execute_generated_flow(user_id, message, context, flow_config, agent_name)
			
			return result
		except Exception as e:
			logger.error(f"è§„åˆ’èŠ‚ç‚¹ {self.id} æ‰§è¡Œå¤±è´¥: {str(e)}")
			error_msg = f"è§„åˆ’èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {str(e)}"
			return self._create_agent_message(error_msg, agent_name, metadata={'error': str(e)})
	
	async def execute_stream(
		self,
		user_id: str,
		message: str,
		context: Dict[str, Any],
		agent_name: str = None
	) -> AsyncGenerator[StreamChunk, None]:
		"""æ‰§è¡Œè§„åˆ’èŠ‚ç‚¹ï¼ˆæµå¼ï¼‰"""
		try:
			# 1. ç”Ÿæˆæµç¨‹å›¾é…ç½®
			yield self._create_stream_chunk(
				chunk_type="content",
				content="ğŸ“‹ æ­£åœ¨è§„åˆ’æµç¨‹å›¾...\n",
				agent_name=agent_name
			)
			
			flow_config = await self._generate_flow_config(message, context)
			if not flow_config:
				error_msg = "è§„åˆ’èŠ‚ç‚¹ï¼šç”Ÿæˆæµç¨‹å›¾é…ç½®å¤±è´¥"
				logger.error(error_msg)
				yield self._create_stream_chunk(
					chunk_type="content",
					content=f"âŒ {error_msg}\n",
					agent_name=agent_name,
					is_end=True
				)
				return
			
			# è¾“å‡ºç”Ÿæˆçš„æµç¨‹å›¾ä¿¡æ¯
			flow_name = flow_config.get('metadata', {}).get('name', 'æœªå‘½åæµç¨‹å›¾')
			generated_nodes = flow_config.get('nodes', [])
			node_count = len(generated_nodes)
			yield self._create_stream_chunk(
				chunk_type="content",
				content=f"âœ… å·²ç”Ÿæˆ {node_count} ä¸ªèŠ‚ç‚¹ï¼š{flow_name}\n\n",
				agent_name=agent_name
			)
			
			# å°†ç”Ÿæˆçš„èŠ‚ç‚¹é…ç½®ä¿å­˜åˆ° flow_stateï¼Œä»¥ä¾¿å‰ç«¯å®æ—¶æ›´æ–°
			flow_state = self._get_flow_state(context)
			flow_state['planner_generated_nodes'] = generated_nodes
			flow_state['planner_generated_edges'] = flow_config.get('edges', [])
			logger.info(f"è§„åˆ’èŠ‚ç‚¹ {self.id} å·²å°†ç”Ÿæˆçš„èŠ‚ç‚¹é…ç½®ä¿å­˜åˆ° flow_stateï¼Œå…± {node_count} ä¸ªèŠ‚ç‚¹")
			
			# è·å–è§„åˆ’èŠ‚ç‚¹çš„åŸå§‹ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ï¼ˆè§„åˆ’èŠ‚ç‚¹åœ¨åŸå§‹æµç¨‹å›¾ä¸­çš„ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ï¼‰
			planner_next_node_id = self.get_next_node_id(0) if self.connections else None
			
			# ä¸´æ—¶æ¸…ç©ºè§„åˆ’èŠ‚ç‚¹çš„ connectionsï¼Œç§»é™¤åˆ°åŸå§‹ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„è¿æ¥
			# è¿™æ · FlowEngine å°±ä¸ä¼šç»§ç»­æ‰§è¡ŒåŸå§‹çš„ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ï¼Œè€Œæ˜¯ç­‰å¾…ç”Ÿæˆçš„èŠ‚ç‚¹æ‰§è¡Œå®Œ
			original_connections = self.connections.copy() if self.connections else []
			self.connections = []  # æ¸…ç©ºè¿æ¥ï¼Œé¿å…ç›´æ¥è¿æ¥åˆ°åŸå§‹ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
			logger.info(f"è§„åˆ’èŠ‚ç‚¹ {self.id} ä¸´æ—¶æ¸…ç©º connectionsï¼ŒåŸå§‹è¿æ¥: {original_connections}")
			
			# å‘é€èŠ‚ç‚¹æ‰©å±•äº‹ä»¶ç»™å‰ç«¯ï¼ˆæ·»åŠ åˆ°ç°æœ‰æµç¨‹å›¾ï¼Œè€Œä¸æ˜¯æ›¿æ¢ï¼‰
			# ä¸å†è¦æ±‚å‰ç«¯åˆ é™¤åŸæœ‰è§„åˆ’èŠ‚ç‚¹åˆ°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„è¾¹ï¼Œåªè¿½åŠ æ–°å­æµç¨‹ç»“æ„ã€‚
			yield self._create_stream_chunk(
				chunk_type="flow_nodes_extend",
				content="",
				agent_name=agent_name,
				metadata={
					'planner_node_id': self.id,
					'planner_next_node_id': planner_next_node_id,  # è§„åˆ’èŠ‚ç‚¹çš„åŸå§‹ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
					'nodes': generated_nodes,
					'edges': flow_config.get('edges', []),
					'flow_name': flow_name,
					'node_count': node_count
				}
			)
			
			# 2. æ‰§è¡Œç”Ÿæˆçš„èŠ‚ç‚¹ï¼ˆæµå¼ï¼‰
			async for chunk in self._execute_generated_nodes_stream(
				user_id, message, context, generated_nodes,
				flow_config.get('edges', []), planner_next_node_id, agent_name
			):
				yield chunk
				
		except Exception as e:
			logger.error(f"è§„åˆ’èŠ‚ç‚¹ {self.id} æµå¼æ‰§è¡Œå¤±è´¥: {str(e)}")
			error_msg = f"è§„åˆ’èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥: {str(e)}"
			yield self._create_stream_chunk(
				chunk_type="content",
				content=f"âŒ {error_msg}\n",
				agent_name=agent_name,
				is_end=True
			)
	
	def _format_failed_nodes_summary(self, failed_nodes: List[Dict[str, str]]) -> str:
		"""æ ¼å¼åŒ–å¤±è´¥èŠ‚ç‚¹æ‘˜è¦"""
		if not failed_nodes:
			return ""
		
		summary_parts = ["æ‰§è¡Œå¤±è´¥çš„èŠ‚ç‚¹ï¼š"]
		for i, failed in enumerate(failed_nodes, 1):
			summary_parts.append(f"{i}. èŠ‚ç‚¹ {failed['node_name']} ({failed['node_id']}): {failed['error']}")
		
		return "\n".join(summary_parts)
	
	async def _generate_flow_config_with_errors(
		self, 
		task: str, 
		context: Dict[str, Any], 
		error_summary: str,
		retry_index: int,
	) -> Optional[Dict[str, Any]]:
		"""ç”ŸæˆåŒ…å«é”™è¯¯ä¿¡æ¯çš„æµç¨‹å›¾é…ç½®ï¼ˆç”¨äºé‡æ–°è§„åˆ’ï¼‰"""
		try:
			# è·å–å¯ç”¨å·¥å…·åˆ—è¡¨
			available_tools = await self._get_available_tools()
			
			# å‡†å¤‡ä¸Šä¸‹æ–‡ä¿¡æ¯
			context_info = self._format_context_info(context)
			
			# æ„å»ºé”™è¯¯ä¸Šä¸‹æ–‡
			error_context = f"""
æ‰§è¡Œé”™è¯¯ä¿¡æ¯ï¼š
{error_summary}

è¯·æ ¹æ®ä»¥ä¸Šé”™è¯¯ä¿¡æ¯ï¼Œé‡æ–°è§„åˆ’æµç¨‹å›¾ï¼Œé¿å…ä¹‹å‰çš„é”™è¯¯ï¼š
1. åˆ†æå¤±è´¥èŠ‚ç‚¹çš„é”™è¯¯åŸå› 
2. è°ƒæ•´èŠ‚ç‚¹é…ç½®æˆ–æ›¿æ¢ä¸ºå…¶ä»–å·¥å…·/æ–¹æ³•
3. ç¡®ä¿æ–°è§„åˆ’çš„èŠ‚ç‚¹èƒ½å¤ŸæˆåŠŸæ‰§è¡Œ
4. å¦‚æœå·¥å…·è°ƒç”¨å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å…¶ä»–å·¥å…·æˆ–è°ƒæ•´å‚æ•°
"""
			
			# æ„å»ºæç¤ºè¯
			system_prompt = self.config.get('system_prompt') or PLANNER_SYSTEM_PROMPT
			user_prompt_template = self.config.get('user_prompt') or PLANNER_USER_PROMPT_TEMPLATE
			user_prompt = user_prompt_template.format(
				task=task,
				context=context_info,
				error_context=error_context,
				available_tools=available_tools,
				planner_id=self.id,
				retry_index=retry_index,
			)
			
			# è°ƒç”¨ LLM
			llm_helper = get_llm_helper()
			messages = [
				{"role": "system", "content": system_prompt},
				{"role": "user", "content": user_prompt}
			]
			
			response = await llm_helper.call(messages, max_tokens=4000)
			
			# è§£æ JSON
			flow_config = self._parse_flow_config(response)
			
			if flow_config:
				# ä¸ºé‡æ–°è§„åˆ’å¾—åˆ°çš„æµç¨‹å›¾ä¹Ÿåšä¸€æ¬¡å‚æ•°æ¨ç†èŠ‚ç‚¹å…œåº•å¤„ç†
				flow_config = self._ensure_auto_infer_edges(flow_config)
				logger.info(
					f"è§„åˆ’èŠ‚ç‚¹ {self.id} é‡æ–°è§„åˆ’æˆåŠŸï¼ŒåŒ…å« {len(flow_config.get('nodes', []))} ä¸ªèŠ‚ç‚¹"
				)
			
			return flow_config
		except Exception as e:
			logger.error(f"è§„åˆ’èŠ‚ç‚¹ {self.id} é‡æ–°è§„åˆ’å¤±è´¥: {str(e)}")
			return None
	
	def _find_last_node_id(
		self,
		nodes: List[Dict[str, Any]],
		edges: List[Dict[str, Any]]
	) -> Optional[str]:
		"""å¯»æ‰¾æ²¡æœ‰å‡ºè¾¹çš„æœ€åä¸€ä¸ªèŠ‚ç‚¹"""
		if not nodes:
			return None
		
		node_ids = [node.get('id') for node in nodes if node.get('id')]
		if not node_ids:
			return None
		
		outgoing_nodes = set()
		for edge in edges or []:
			source = edge.get('source')
			if source:
				outgoing_nodes.add(source)
		
		for node_id in reversed(node_ids):
			if node_id not in outgoing_nodes:
				return node_id
		
		return node_ids[-1]
	
	def _build_display_flow_with_virtual_end(
		self,
		nodes: List[Dict[str, Any]],
		edges: List[Dict[str, Any]],
		last_node_id: Optional[str]
	) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
		"""
		æ ¹æ®ç”Ÿæˆçš„èŠ‚ç‚¹å’Œè¾¹ï¼Œæ„å»ºç”¨äºå‰ç«¯å±•ç¤ºçš„æµç¨‹å›¾ã€‚
		
		æ³¨æ„ï¼šä¸ºäº†ä¿è¯æ‰€æœ‰ä¸åŒè·¯çº¿æœ€ç»ˆæ±‡èšåˆ°å…¨å±€å”¯ä¸€çš„ç»“æŸèŠ‚ç‚¹ï¼Œ
		è¿™é‡Œä¸å†åœ¨å­æµç¨‹å†…éƒ¨åˆ›å»ºç‹¬ç«‹çš„è™šæ‹Ÿ end èŠ‚ç‚¹ï¼Œä»…é€ä¼ åŸæœ‰ nodes/edgesã€‚
		"""
		display_nodes = list(nodes)
		display_edges = list(edges)
		return display_nodes, display_edges
	
	def _build_retry_flow_display_nodes(
		self,
		root_planner_id: str,
		retry_index: int,
		nodes: List[Dict[str, Any]],
		edges: List[Dict[str, Any]],
		last_node_id: Optional[str]
	) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], str]:
		"""
		æ„å»ºâ€œé‡æ–°è§„åˆ’èŠ‚ç‚¹ + æ–°å­æµç¨‹â€çš„å±•ç¤ºç»“æ„ï¼š
		- åœ¨åŸè§„åˆ’èŠ‚ç‚¹ä¸‹æ–¹æ–°å¢ä¸€ä¸ªè™šæ‹Ÿçš„ retry èŠ‚ç‚¹
		- ä»åŸè§„åˆ’èŠ‚ç‚¹è¿æ¥åˆ° retry èŠ‚ç‚¹
		- ä» retry èŠ‚ç‚¹è¿æ¥åˆ°æ–°å­æµç¨‹çš„èµ·å§‹èŠ‚ç‚¹
		- å­æµç¨‹æœ«å°¾ä¸å†åˆ›å»ºç‹¬ç«‹çš„è™šæ‹Ÿ end èŠ‚ç‚¹ï¼Œè€Œæ˜¯ç»Ÿä¸€è¿æ¥åˆ°å…¨å±€ end_node
		
		è¿”å›ï¼šdisplay_nodes, display_edges, retry_node_id
		"""
		# å­æµç¨‹å†…éƒ¨ä»…ä½¿ç”¨è‡ªèº«çš„ nodes/edgesï¼Œä¸å†åˆ›å»ºè™šæ‹Ÿ end èŠ‚ç‚¹
		child_nodes, child_edges = self._build_display_flow_with_virtual_end(nodes, edges, last_node_id)

		retry_node_id = f"{root_planner_id}_retry_{retry_index}"
		retry_label = "é‡æ–°è§„åˆ’" if retry_index == 1 else f"é‡æ–°è§„åˆ’ {retry_index} æ¬¡"

		retry_node = {
			'id': retry_node_id,
			'type': 'planner_retry',
			'nodeType': 'planner_retry',
			'data': {
				'label': retry_label,
				'nodeType': 'planner_retry'
			}
		}

		# è®¡ç®—å­æµç¨‹èµ·å§‹èŠ‚ç‚¹ï¼ˆå…¥åº¦ä¸º 0 çš„èŠ‚ç‚¹ï¼‰
		node_ids = [n.get('id') for n in nodes if n.get('id')]
		target_ids = {e.get('target') for e in edges or [] if e.get('target')}
		start_node_id: Optional[str] = None
		for nid in node_ids:
			if nid not in target_ids:
				start_node_id = nid
				break
		if not start_node_id and node_ids:
			start_node_id = node_ids[0]

		display_nodes = [retry_node] + child_nodes
		display_edges = list(child_edges)

		# ä»åŸè§„åˆ’èŠ‚ç‚¹è¿æ¥åˆ° retry èŠ‚ç‚¹ï¼ˆä¿è¯ retry èŠ‚ç‚¹ä¸æ˜¯å­¤å„¿èŠ‚ç‚¹ï¼‰
		display_edges.append({
			'id': f"edge_{root_planner_id}_{retry_node_id}",
			'source': root_planner_id,
			'target': retry_node_id,
			'type': 'default'
		})

		# ä» retry èŠ‚ç‚¹è¿æ¥åˆ°æ–°å­æµç¨‹èµ·å§‹èŠ‚ç‚¹ï¼ˆä¿è¯å­æµç¨‹ä¸ retry ç›¸è¿ï¼‰
		if start_node_id:
			display_edges.append({
				'id': f"edge_{retry_node_id}_{start_node_id}",
				'source': retry_node_id,
				'target': start_node_id,
				'type': 'default'
			})

		# æ‰€æœ‰ä¸åŒçš„è·¯çº¿æœ€ç»ˆç»Ÿä¸€è¿æ¥åˆ°å…¨å±€å”¯ä¸€çš„ç»“æŸèŠ‚ç‚¹ end_node
		global_end_id = "end_node"
		if last_node_id:
			display_edges.append({
				'id': f"edge_{last_node_id}_{global_end_id}",
				'source': last_node_id,
				'target': global_end_id,
				'type': 'default'
			})

		return display_nodes, display_edges, retry_node_id
	
	def _ensure_auto_infer_edges(self, flow_config: Dict[str, Any]) -> Dict[str, Any]:
		"""
		åç«¯å…œåº•ï¼šä¿è¯å‚æ•°æ¨ç†èŠ‚ç‚¹ï¼ˆauto_inferï¼‰è‡³å°‘æœ‰ä¸€æ¡å‡ºè¾¹æŒ‡å‘ç›®æ ‡å·¥å…·èŠ‚ç‚¹ï¼Œ
		å¦åˆ™å°†å…¶ä»æµç¨‹å›¾ä¸­ç§»é™¤ï¼Œé¿å…åœ¨å‰ç«¯å‡ºç°å®Œå…¨æ²¡æœ‰ä¸Šä¸‹æ–‡çš„å­¤ç«‹èŠ‚ç‚¹ã€‚
		"""
		if not flow_config:
			return flow_config
		
		nodes = flow_config.get("nodes", []) or []
		edges = flow_config.get("edges", []) or []
		
		# æ–¹ä¾¿æŸ¥æ‰¾èŠ‚ç‚¹ä¸è¾¹
		node_map: Dict[str, Dict[str, Any]] = {}
		for n in nodes:
			nid = n.get("id")
			if nid:
				node_map[nid] = n
		
		outgoing_by_source: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
		for e in edges:
			src = e.get("source")
			if src:
				outgoing_by_source[src].append(e)
		
		new_edges = list(edges)
		nodes_to_keep: List[Dict[str, Any]] = []
		
		for n in nodes:
			nid = n.get("id")
			if not nid:
				continue
			
			# åˆ¤æ–­æ˜¯å¦æ˜¯ auto_infer èŠ‚ç‚¹
			raw_type = (n.get("type") or "").lower()
			data_node_type = (n.get("data", {}).get("nodeType") or "").lower()
			is_auto_infer = ("auto_infer" in raw_type) or ("auto_infer" in data_node_type)
			
			if not is_auto_infer:
				nodes_to_keep.append(n)
				continue
			
			# å·²ç»æœ‰å‡ºè¾¹åˆ™è®¤ä¸ºä¸æ˜¯å­¤ç«‹çš„ï¼ˆè‡³å°‘è¿æ¥åˆ°åˆ«çš„èŠ‚ç‚¹ï¼‰
			has_outgoing = nid in outgoing_by_source and len(outgoing_by_source[nid]) > 0
			
			if not has_outgoing:
				# å°è¯•ä» config ä¸­è¯»å–ç›®æ ‡å·¥å…·èŠ‚ç‚¹ IDï¼Œå¹¶è¡¥ä¸€æ¡å‡ºè¾¹
				config = n.get("data", {}).get("config", {}) or {}
				target_id = (
					config.get("target_tool_node_id")
					or config.get("targetNodeId")
					or config.get("target_tool_id")
				)
				
				if target_id and target_id in node_map:
					edge_id = f"edge_{nid}_{target_id}"
					new_edge = {
						"id": edge_id,
						"source": nid,
						"target": target_id,
						"type": "default",
					}
					new_edges.append(new_edge)
					outgoing_by_source[nid].append(new_edge)
					has_outgoing = True
					logger.info(
						f"è§„åˆ’èŠ‚ç‚¹ {self.id} ä¸ºå‚æ•°æ¨ç†èŠ‚ç‚¹ {nid} è‡ªåŠ¨è¡¥å……å‡ºè¾¹ -> {target_id}ï¼Œé¿å…å­¤ç«‹"
					)
			
			# å¦‚æœæœ€ç»ˆä»ç„¶æ²¡æœ‰ä»»ä½•å‡ºè¾¹ï¼Œåˆ™è®¤ä¸ºæ˜¯â€œæ— æ³•æ­£ç¡®æŒ‚è½½çš„å­¤å„¿èŠ‚ç‚¹â€ï¼Œç›´æ¥ä¸¢å¼ƒ
			if has_outgoing:
				nodes_to_keep.append(n)
			else:
				logger.warning(
					f"è§„åˆ’èŠ‚ç‚¹ {self.id} æ£€æµ‹åˆ°å­¤ç«‹å‚æ•°æ¨ç†èŠ‚ç‚¹ {nid}ï¼Œä¸”æ— æ³•ç¡®å®šç›®æ ‡å·¥å…·èŠ‚ç‚¹ï¼Œå·²ä»æµç¨‹å›¾ä¸­ç§»é™¤"
				)
		
		flow_config["nodes"] = nodes_to_keep
		flow_config["edges"] = new_edges
		return flow_config

	def _namespace_flow_nodes_for_retry(
		self,
		flow_config: Dict[str, Any],
		retry_index: int
	) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
		"""
		ä¸ºé‡æ–°è§„åˆ’å‡ºæ¥çš„èŠ‚ç‚¹ç”Ÿæˆç‹¬ç«‹çš„ ID å‘½åç©ºé—´ï¼š
		- æ¯ä¸ªèŠ‚ç‚¹ ID åŠ ä¸Šå‰ç¼€: {planner_id}_retry_{retry_index}_åŸID
		- åŒæ—¶ä¿®æ­£ edges ä¸­çš„ source/target
		- ä¿®æ­£èŠ‚ç‚¹ config ä¸­å¼•ç”¨å…¶å®ƒèŠ‚ç‚¹ ID çš„å­—æ®µï¼ˆå¦‚ target_tool_node_idï¼‰
		
		è¿™æ ·æ–°è·¯çº¿ä¸Šçš„èŠ‚ç‚¹ä¸è€è·¯çº¿å®Œå…¨ç‹¬ç«‹ï¼Œä¸ä¼šå¤ç”¨ä¹‹å‰çš„ node_idã€‚
		"""
		nodes = flow_config.get("nodes", []) or []
		edges = flow_config.get("edges", []) or []

		id_map: Dict[str, str] = {}
		for n in nodes:
			old_id = n.get("id")
			if not old_id:
				continue
			new_id = f"{self.id}_retry_{retry_index}_{old_id}"
			id_map[old_id] = new_id

		# é‡å†™èŠ‚ç‚¹ ID ä»¥åŠ config ä¸­çš„ç›®æ ‡èŠ‚ç‚¹å¼•ç”¨
		new_nodes: List[Dict[str, Any]] = []
		for n in nodes:
			old_id = n.get("id")
			if not old_id:
				continue
			n_copy = json.loads(json.dumps(n))  # æ·±æ‹·è´ä»¥é¿å…ä¿®æ”¹åŸé…ç½®
			n_copy["id"] = id_map.get(old_id, old_id)

			# ä¿®æ­£ data.config ä¸­å¯èƒ½å¼•ç”¨å…¶å®ƒèŠ‚ç‚¹ ID çš„å­—æ®µ
			data = n_copy.get("data") or {}
			config = data.get("config") or {}
			changed = False
			for key in ("target_tool_node_id", "targetNodeId", "target_tool_id"):
				ref_id = config.get(key)
				if isinstance(ref_id, str) and ref_id in id_map:
					config[key] = id_map[ref_id]
					changed = True

			# é‡è¯•åˆ†æ”¯ï¼šä¸ºèŠ‚ç‚¹ label æ·»åŠ â€œé‡è¯•â€åç¼€ï¼Œæ–¹ä¾¿å‰ç«¯åŒºåˆ†ä¸åŒçº¿è·¯
			if retry_index > 0:
				label = data.get("label") or data.get("nodeType") or n_copy.get("type") or old_id
				retry_suffix = f"é‡è¯•{retry_index}" if retry_index > 1 else "é‡è¯•1"
				data["label"] = f"{label} ({retry_suffix})"
				changed = True

			if changed:
				data["config"] = config
				n_copy["data"] = data

			new_nodes.append(n_copy)

		# é‡å†™è¾¹çš„ source/targetï¼Œä»…ä¿ç•™â€œå®Œå…¨åœ¨æœ¬å­å›¾å†…éƒ¨â€çš„è¾¹
		new_edges: List[Dict[str, Any]] = []
		for e in edges:
			e_copy = dict(e)
			src = e_copy.get("source")
			tgt = e_copy.get("target")
			# åªä¿ç•™ source å’Œ target éƒ½å±äºå½“å‰é‡è¯•å­å›¾èŠ‚ç‚¹çš„è¾¹ï¼Œä¸¢å¼ƒæŒ‡å‘è€èŠ‚ç‚¹çš„è¾¹
			if src not in id_map or tgt not in id_map:
				continue
			e_copy["source"] = id_map[src]
			e_copy["target"] = id_map[tgt]
			# ä¸ºé¿å…ä¸æ—§è·¯çº¿çš„è¾¹ ID å†²çªï¼Œé‡è¯•å­æµç¨‹çš„æ¯æ¡è¾¹éƒ½ä½¿ç”¨åŸºäºæ–° source/target çš„å”¯ä¸€ ID
			e_copy["id"] = f"edge_{e_copy.get('source')}_{e_copy.get('target')}"
			new_edges.append(e_copy)

		# å…œåº•ï¼šå¦‚æœå­å›¾å†…éƒ¨ä»ç„¶æœ‰â€œæ–­é“¾â€ï¼ŒæŒ‰èŠ‚ç‚¹é¡ºåºè¡¥ä¸€æ¡é“¾å¼è¾¹ï¼Œä¿è¯é‡è¯•å­å›¾è¿æˆä¸€æ¡è·¯
		if new_nodes:
			existing_pairs = {(e["source"], e["target"]) for e in new_edges}
			ordered_ids = [n["id"] for n in new_nodes]
			for i in range(len(ordered_ids) - 1):
				src_id = ordered_ids[i]
				tgt_id = ordered_ids[i + 1]
				if (src_id, tgt_id) not in existing_pairs:
					edge_id = f"edge_{src_id}_{tgt_id}"
					new_edges.append({
						"id": edge_id,
						"source": src_id,
						"target": tgt_id,
						"type": "default",
					})
					existing_pairs.add((src_id, tgt_id))

		return new_nodes, new_edges
	
	async def _generate_flow_config(self, task: str, context: Dict[str, Any]) -> Optional[Dict[str, Any]]:
		"""ä½¿ç”¨ LLM ç”Ÿæˆæµç¨‹å›¾é…ç½®"""
		try:
			# è·å–å¯ç”¨å·¥å…·åˆ—è¡¨
			available_tools = await self._get_available_tools()
			
			# å‡†å¤‡ä¸Šä¸‹æ–‡ä¿¡æ¯
			context_info = self._format_context_info(context)
			
			# æ„å»ºæç¤ºè¯
			system_prompt = self.config.get('system_prompt') or PLANNER_SYSTEM_PROMPT
			user_prompt_template = self.config.get('user_prompt') or PLANNER_USER_PROMPT_TEMPLATE
			user_prompt = user_prompt_template.format(
				task=task,
				context=context_info,
				error_context="",  # é¦–æ¬¡è§„åˆ’æ—¶æ²¡æœ‰é”™è¯¯ä¿¡æ¯
				available_tools=available_tools,
				planner_id=self.id,
				retry_index=0,
			)
			
			# è°ƒç”¨ LLM
			llm_helper = get_llm_helper()
			messages = [
				{"role": "system", "content": system_prompt},
				{"role": "user", "content": user_prompt}
			]
			
			response = await llm_helper.call(messages, max_tokens=4000)
			
			# è§£æ JSON
			flow_config = self._parse_flow_config(response)
			
			if flow_config:
				# å…ˆä¸ºå‚æ•°æ¨ç†èŠ‚ç‚¹å…œåº•è¡¥è¾¹ / è¿‡æ»¤å­¤ç«‹ auto_infer èŠ‚ç‚¹
				flow_config = self._ensure_auto_infer_edges(flow_config)
				self._generated_flow_config = flow_config
				logger.info(
					f"è§„åˆ’èŠ‚ç‚¹ {self.id} æˆåŠŸç”Ÿæˆæµç¨‹å›¾é…ç½®ï¼ŒåŒ…å« {len(flow_config.get('nodes', []))} ä¸ªèŠ‚ç‚¹"
				)
			
			return flow_config
		except Exception as e:
			logger.error(f"è§„åˆ’èŠ‚ç‚¹ {self.id} ç”Ÿæˆæµç¨‹å›¾é…ç½®å¤±è´¥: {str(e)}")
			return None
	
	def _parse_flow_config(self, text: str) -> Optional[Dict[str, Any]]:
		"""ä» LLM å“åº”ä¸­è§£ææµç¨‹å›¾é…ç½®"""
		if not text:
			return None
		
		try:
			# å°è¯•æå– JSONï¼ˆå¯èƒ½åœ¨ä»£ç å—ä¸­ï¼‰
			clean = text.strip()
			
			# å¦‚æœåŒ…å« ```json æˆ– ```ï¼Œæå–å…¶ä¸­çš„å†…å®¹
			if "```json" in clean:
				start = clean.find("```json") + 7
				end = clean.find("```", start)
				if end > start:
					clean = clean[start:end].strip()
			elif "```" in clean:
				start = clean.find("```") + 3
				end = clean.find("```", start)
				if end > start:
					clean = clean[start:end].strip()
			
			# è§£æ JSON
			config = json.loads(clean)
			
			# éªŒè¯é…ç½®ç»“æ„
			if not isinstance(config, dict):
				logger.error("æµç¨‹å›¾é…ç½®ä¸æ˜¯å­—å…¸æ ¼å¼")
				return None
			
			if 'nodes' not in config:
				logger.error("æµç¨‹å›¾é…ç½®ç¼ºå°‘ nodes å­—æ®µ")
				return None
			
			# è¿‡æ»¤æ‰ start å’Œ end èŠ‚ç‚¹
			nodes = config.get('nodes', [])
			filtered_nodes = []
			for node in nodes:
				node_data = node.get('data', {})
				node_type = node.get('type', '')
				# è·³è¿‡å¼€å§‹å’Œç»“æŸèŠ‚ç‚¹
				if (node_data.get('isStartNode') or node_type == 'start' or 
				    node_data.get('isEndNode') or node_type == 'end' or
				    node.get('id') == 'start_node' or node.get('id') == 'end_node'):
					continue
				filtered_nodes.append(node)
			
			config['nodes'] = filtered_nodes
			
			# è¿‡æ»¤æ‰è¿æ¥åˆ° start æˆ– end èŠ‚ç‚¹çš„è¾¹
			edges = config.get('edges', [])
			filtered_edges = []
			start_end_ids = {'start_node', 'end_node'}
			for edge in edges:
				source = edge.get('source', '')
				target = edge.get('target', '')
				# è·³è¿‡è¿æ¥åˆ° start æˆ– end èŠ‚ç‚¹çš„è¾¹
				if source in start_end_ids or target in start_end_ids:
					continue
				filtered_edges.append(edge)
			
			config['edges'] = filtered_edges
			
			# éªŒè¯å¹¶ä¿®å¤èŠ‚ç‚¹è¿æ¥ï¼Œç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹ä¸²è¡Œè¿æ¥
			config = self._ensure_serial_connection(config)
			
			# ç¡®ä¿æœ‰ metadata
			if 'metadata' not in config:
				config['metadata'] = {
					"name": "è‡ªåŠ¨ç”Ÿæˆçš„æµç¨‹å›¾",
					"description": "",
					"version": "1.0.0"
				}
			
			return config
		except json.JSONDecodeError as e:
			logger.error(f"è§£ææµç¨‹å›¾é…ç½® JSON å¤±è´¥: {str(e)}")
			logger.debug(f"åŸå§‹å“åº”: {text[:500]}")
			return None
		except Exception as e:
			logger.error(f"è§£ææµç¨‹å›¾é…ç½®å¤±è´¥: {str(e)}")
			return None
	
	def _generate_edges_from_nodes(self, nodes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
		"""æ ¹æ®èŠ‚ç‚¹åˆ—è¡¨è‡ªåŠ¨ç”Ÿæˆè¾¹è¿æ¥ï¼ˆä¸åŒ…å« start å’Œ end èŠ‚ç‚¹ï¼‰"""
		edges = []
		
		if len(nodes) < 2:
			return edges
		
		# èŠ‚ç‚¹ä¾æ¬¡è¿æ¥
		for i in range(len(nodes) - 1):
			edges.append({
				"id": f"edge_{nodes[i].get('id')}_{nodes[i+1].get('id')}",
				"source": nodes[i].get('id'),
				"target": nodes[i+1].get('id'),
				"type": "default"
			})
		
		return edges
	
	def _ensure_serial_connection(self, config: Dict[str, Any]) -> Dict[str, Any]:
		"""ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹ä¸²è¡Œè¿æ¥ï¼Œç§»é™¤æ¸¸ç¦»èŠ‚ç‚¹å’Œåˆ†æ”¯"""
		nodes = config.get('nodes', [])
		edges = config.get('edges', [])
		
		if not nodes:
			config['edges'] = []
			return config
		
		if len(nodes) == 1:
			# åªæœ‰ä¸€ä¸ªèŠ‚ç‚¹ï¼Œä¸éœ€è¦è¾¹
			config['edges'] = []
			return config
		
		# æ„å»ºèŠ‚ç‚¹IDåˆ°èŠ‚ç‚¹çš„æ˜ å°„
		node_map = {node.get('id'): node for node in nodes}
		node_ids = list(node_map.keys())
		
		# æ„å»ºå…¥åº¦å’Œå‡ºåº¦ç»Ÿè®¡
		in_degree = {node_id: 0 for node_id in node_ids}
		out_degree = {node_id: 0 for node_id in node_ids}
		edge_map = {}  # source -> [targets]
		
		for edge in edges:
			source = edge.get('source')
			target = edge.get('target')
			if source in node_map and target in node_map:
				if source not in edge_map:
					edge_map[source] = []
				edge_map[source].append(target)
				out_degree[source] = out_degree.get(source, 0) + 1
				in_degree[target] = in_degree.get(target, 0) + 1
		
		# æ£€æŸ¥æ˜¯å¦æœ‰åˆ†æ”¯æˆ–æ¸¸ç¦»èŠ‚ç‚¹
		has_branch = False
		orphan_nodes = []
		
		# æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªå‡ºè¾¹ï¼ˆåˆ†æ”¯ï¼‰
		for node_id, out_count in out_degree.items():
			if out_count > 1:
				has_branch = True
				logger.warning(f"èŠ‚ç‚¹ {node_id} æœ‰ {out_count} ä¸ªå‡ºè¾¹ï¼Œå­˜åœ¨åˆ†æ”¯")
		
		# æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªå…¥è¾¹ï¼ˆåˆå¹¶ï¼‰
		for node_id, in_count in in_degree.items():
			if in_count > 1:
				has_branch = True
				logger.warning(f"èŠ‚ç‚¹ {node_id} æœ‰ {in_count} ä¸ªå…¥è¾¹ï¼Œå­˜åœ¨åˆå¹¶")
		
		# æ£€æŸ¥æ¸¸ç¦»èŠ‚ç‚¹ï¼ˆæ²¡æœ‰å…¥è¾¹ä¹Ÿæ²¡æœ‰å‡ºè¾¹ï¼‰
		for node_id in node_ids:
			in_count = in_degree.get(node_id, 0)
			out_count = out_degree.get(node_id, 0)
			# ç¬¬ä¸€ä¸ªèŠ‚ç‚¹åº”è¯¥æ²¡æœ‰å…¥è¾¹ä½†æœ‰å‡ºè¾¹ï¼Œæœ€åä¸€ä¸ªèŠ‚ç‚¹åº”è¯¥æ²¡æœ‰å‡ºè¾¹ä½†æœ‰å…¥è¾¹
			# ä¸­é—´èŠ‚ç‚¹åº”è¯¥éƒ½æœ‰å…¥è¾¹å’Œå‡ºè¾¹
			# å¦‚æœèŠ‚ç‚¹æ—¢æ²¡æœ‰å…¥è¾¹ä¹Ÿæ²¡æœ‰å‡ºè¾¹ï¼Œå°±æ˜¯æ¸¸ç¦»èŠ‚ç‚¹
			if in_count == 0 and out_count == 0:
				orphan_nodes.append(node_id)
				logger.warning(f"èŠ‚ç‚¹ {node_id} æ˜¯æ¸¸ç¦»èŠ‚ç‚¹ï¼ˆæ²¡æœ‰è¿æ¥ï¼‰")
		
		# å¦‚æœæœ‰åˆ†æ”¯æˆ–æ¸¸ç¦»èŠ‚ç‚¹ï¼Œé‡æ–°ç”Ÿæˆä¸²è¡Œè¿æ¥
		if has_branch or orphan_nodes:
			logger.info(f"æ£€æµ‹åˆ°éä¸²è¡Œè¿æ¥ï¼Œé‡æ–°ç”Ÿæˆä¸²è¡Œè¾¹ã€‚åˆ†æ”¯: {has_branch}, æ¸¸ç¦»èŠ‚ç‚¹: {orphan_nodes}")
			
			# ç§»é™¤æ¸¸ç¦»èŠ‚ç‚¹
			if orphan_nodes:
				nodes = [node for node in nodes if node.get('id') not in orphan_nodes]
				node_ids = [node.get('id') for node in nodes]
				logger.info(f"ç§»é™¤äº† {len(orphan_nodes)} ä¸ªæ¸¸ç¦»èŠ‚ç‚¹ï¼Œå‰©ä½™ {len(nodes)} ä¸ªèŠ‚ç‚¹")
			
			# é‡æ–°ç”Ÿæˆä¸²è¡Œè¾¹
			new_edges = []
			for i in range(len(nodes) - 1):
				source_id = nodes[i].get('id')
				target_id = nodes[i+1].get('id')
				new_edges.append({
					"id": f"edge_{source_id}_{target_id}",
					"source": source_id,
					"target": target_id,
					"type": "default"
				})
			
			config['nodes'] = nodes
			config['edges'] = new_edges
			logger.info(f"é‡æ–°ç”Ÿæˆäº† {len(new_edges)} æ¡ä¸²è¡Œè¾¹")
		else:
			# éªŒè¯æ˜¯å¦æ‰€æœ‰èŠ‚ç‚¹éƒ½åœ¨è·¯å¾„ä¸Š
			# æ‰¾åˆ°èµ·å§‹èŠ‚ç‚¹ï¼ˆå…¥åº¦ä¸º0ï¼‰
			start_nodes = [node_id for node_id, in_count in in_degree.items() if in_count == 0]
			if len(start_nodes) != 1:
				logger.warning(f"èµ·å§‹èŠ‚ç‚¹æ•°é‡ä¸æ­£ç¡®: {len(start_nodes)}ï¼ŒæœŸæœ›1ä¸ª")
				# é‡æ–°ç”Ÿæˆä¸²è¡Œè¾¹
				new_edges = []
				for i in range(len(nodes) - 1):
					source_id = nodes[i].get('id')
					target_id = nodes[i+1].get('id')
					new_edges.append({
						"id": f"edge_{source_id}_{target_id}",
						"source": source_id,
						"target": target_id,
						"type": "default"
					})
				config['edges'] = new_edges
			else:
				# éªŒè¯è·¯å¾„å®Œæ•´æ€§ï¼šä»èµ·å§‹èŠ‚ç‚¹å¼€å§‹ï¼Œæ£€æŸ¥æ˜¯å¦èƒ½åˆ°è¾¾æ‰€æœ‰èŠ‚ç‚¹
				visited = set()
				start_node_id = start_nodes[0]
				current = start_node_id
				
				while current and current not in visited:
					visited.add(current)
					# è·å–ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
					next_nodes = edge_map.get(current, [])
					if len(next_nodes) > 1:
						# æœ‰åˆ†æ”¯ï¼Œåªå–ç¬¬ä¸€ä¸ª
						logger.warning(f"èŠ‚ç‚¹ {current} æœ‰å¤šä¸ªåç»§ï¼Œåªä¿ç•™ç¬¬ä¸€ä¸ª")
						next_nodes = [next_nodes[0]]
					current = next_nodes[0] if next_nodes else None
				
				# æ£€æŸ¥æ˜¯å¦æœ‰æœªè®¿é—®çš„èŠ‚ç‚¹
				unvisited = set(node_ids) - visited
				if unvisited:
					logger.warning(f"æœ‰ {len(unvisited)} ä¸ªèŠ‚ç‚¹ä¸åœ¨è·¯å¾„ä¸Š: {unvisited}")
					# é‡æ–°ç”Ÿæˆä¸²è¡Œè¾¹
					new_edges = []
					for i in range(len(nodes) - 1):
						source_id = nodes[i].get('id')
						target_id = nodes[i+1].get('id')
						new_edges.append({
							"id": f"edge_{source_id}_{target_id}",
							"source": source_id,
							"target": target_id,
							"type": "default"
						})
					config['edges'] = new_edges
		
		return config
	
	async def _get_available_tools(self) -> str:
		"""è·å–ç³»ç»Ÿå†…æ‰€æœ‰å¯ç”¨å·¥å…·åˆ—è¡¨ï¼ˆåŒ…æ‹¬å†…ç½®å·¥å…·ã€MCPå·¥å…·ã€ä¸´æ—¶å·¥å…·ï¼‰
		
		è§„åˆ™ï¼š
		- ToolManager å…ˆæŒ‰è¯„åˆ†ä»é«˜åˆ°ä½æ’åº
		- è¿™é‡Œå†æŒ‰ (type, category) åˆ†ç»„ï¼Œ**æ¯ç»„åªä¿ç•™è¯„åˆ†æœ€é«˜çš„ä¸€ä¸ªå·¥å…·**
		  ä¹Ÿå°±æ˜¯è¯´ï¼šå¤šä¸ªåŠŸèƒ½ç›¸è¿‘ï¼ˆåŒä¸€ç±»å‹+åŒä¸€ç±»åˆ«ï¼‰çš„å·¥å…·æ—¶ï¼Œåªæš´éœ²è¯„åˆ†æœ€é«˜çš„é‚£ä¸ªç»™è§„åˆ’ LLMï¼Œé¿å…ä½åˆ†å·¥å…·è¢«é€‰æ‹©ã€‚
		"""
		try:
			from main import agent_manager
			if not agent_manager or not agent_manager.tool_manager:
				logger.warning("AgentManager æˆ– ToolManager æœªåˆå§‹åŒ–")
				return "æš‚æ— å¯ç”¨å·¥å…·"
			
			tool_manager = agent_manager.tool_manager
			# è·å–æ‰€æœ‰å¯ç”¨å·¥å…·ï¼ˆåŒ…æ‹¬å†…ç½®ã€MCPã€ä¸´æ—¶å·¥å…·ï¼‰ï¼Œå·²æŒ‰è¯„åˆ†ä»é«˜åˆ°ä½æ’åº
			all_tools = tool_manager.get_available_tools()
			
			if not all_tools:
				logger.warning("æœªè·å–åˆ°ä»»ä½•å·¥å…·")
				return "æš‚æ— å¯ç”¨å·¥å…·"
			
			# æŒ‰ (type, category) åˆ†ç»„ï¼Œåªä¿ç•™æ¯ç»„è¯„åˆ†æœ€é«˜çš„ä¸€ä¸ª
			grouped: Dict[Tuple[str, str], List[Dict[str, Any]]] = defaultdict(list)
			for t in all_tools:
				t_type = t.get("type", "unknown")
				t_category = t.get("category", "utility")
				grouped[(t_type, t_category)].append(t)
			
			filtered_tools: List[Dict[str, Any]] = []
			for (t_type, t_category), group in grouped.items():
				# group å·²ç»æ˜¯æ•´ä½“æ’åºä¹‹åçš„åˆ‡ç‰‡ï¼Œä½†ä¸ºç¨³å¦¥å†å±€éƒ¨æ’åºä¸€æ¬¡
				group_sorted = sorted(group, key=lambda x: x.get("score", 1.0), reverse=True)
				best_tool = group_sorted[0]
				filtered_tools.append(best_tool)
				if len(group_sorted) > 1:
					removed_names = [g.get("name", "unknown") for g in group_sorted[1:]]
					logger.info(
						f"è§„åˆ’èŠ‚ç‚¹æŒ‰ (type={t_type}, category={t_category}) åˆ†ç»„ï¼Œåªä¿ç•™è¯„åˆ†æœ€é«˜å·¥å…· "
						f"{best_tool.get('name')} (score={best_tool.get('score')})ï¼Œ"
						f"è¿‡æ»¤æ‰åŒç»„å…¶å®ƒå·¥å…·: {removed_names}"
					)
			
			logger.info(
				f"è§„åˆ’èŠ‚ç‚¹è·å–åˆ° {len(all_tools)} ä¸ªåŸå§‹å·¥å…·ï¼ŒæŒ‰åŠŸèƒ½åˆ†ç»„åä¿ç•™ {len(filtered_tools)} ä¸ªä»£è¡¨å·¥å…·"
			)
			
			# æŒ‰ç±»å‹åˆ†ç»„å·¥å…·ï¼ˆåŸºäºè¿‡æ»¤åçš„åˆ—è¡¨ï¼‰
			tools_by_type: Dict[str, List[Dict[str, Any]]] = {}
			for tool_info in filtered_tools:
				tool_type = tool_info.get('type', 'unknown')
				if tool_type not in tools_by_type:
					tools_by_type[tool_type] = []
				tools_by_type[tool_type].append(tool_info)
			
			# æ ¼å¼åŒ–å·¥å…·ä¿¡æ¯
			tool_sections: List[str] = []
			
			# å†…ç½®å·¥å…·ï¼ˆæ¯ä¸ªåŠŸèƒ½ç±»åˆ«åªä¿ç•™ä¸€ä¸ªä»£è¡¨å·¥å…·ï¼ŒæŒ‰è¯„åˆ†ä»é«˜åˆ°ä½å±•ç¤ºï¼‰
			if 'builtin' in tools_by_type:
				tool_sections.append("## å†…ç½®å·¥å…·ï¼ˆæ¯ä¸ªåŠŸèƒ½ç±»åˆ«åªä¿ç•™è¯„åˆ†æœ€é«˜çš„ä¸€ä¸ªï¼ŒæŒ‰è¯„åˆ†ä»é«˜åˆ°ä½æ’åºï¼‰ï¼š")
				for tool_info in sorted(tools_by_type['builtin'], key=lambda x: x.get('score', 1.0), reverse=True):
					tool_name = tool_info.get('name', 'unknown')
					tool_desc = tool_info.get('description', 'æ— æè¿°')
					params = tool_info.get('parameters', {})
					params_desc = self._format_parameters_schema(params)
					score = tool_info.get('score', 1.0)
					category = tool_info.get('category', 'utility')
					tool_sections.append(f"- **{tool_name}** (ç±»åˆ«: {category}, è¯„åˆ†: {score:.2f}): {tool_desc}")
					if params_desc:
						tool_sections.append(f"  å‚æ•°: {params_desc}")
			
			# MCPå·¥å…·ï¼ˆæ¯ä¸ªæœåŠ¡å™¨+ç±»åˆ«åªä¿ç•™è¯„åˆ†æœ€é«˜çš„ä¸€ä¸ªï¼Œåœ¨æ¯ä¸ªæœåŠ¡å™¨å†…æŒ‰è¯„åˆ†ä»é«˜åˆ°ä½å±•ç¤ºï¼‰
			mcp_tools = [t for t in filtered_tools if t.get('type') == 'mcp']
			if mcp_tools:
				tool_sections.append("\n## MCPå·¥å…·ï¼ˆæŒ‰è¯„åˆ†ä»é«˜åˆ°ä½æ’åºï¼Œä¼˜å…ˆé€‰æ‹©é«˜è¯„åˆ†å·¥å…·ï¼‰ï¼š")
				# æŒ‰æœåŠ¡å™¨åˆ†ç»„
				tools_by_server = {}
				for tool_info in mcp_tools:
					# ä»å·¥å…·åç§°ä¸­æå–æœåŠ¡å™¨åï¼ˆæ ¼å¼ï¼šmcp_{server}_{tool_name}ï¼‰
					tool_name = tool_info.get('name', '')
					if tool_name.startswith('mcp_'):
						parts = tool_name.split('_', 2)
						if len(parts) >= 3:
							server_name = parts[1]
							if server_name not in tools_by_server:
								tools_by_server[server_name] = []
							tools_by_server[server_name].append(tool_info)
				
				for server_name, server_tools in tools_by_server.items():
					# æ¯ä¸ªæœåŠ¡å™¨å†…æŒ‰è¯„åˆ†æ’åº
					server_tools_sorted = sorted(server_tools, key=lambda t: t.get('score', 1.0), reverse=True)
					tool_sections.append(f"\n### æœåŠ¡å™¨ {server_name}ï¼š")
					for tool_info in server_tools_sorted:
						tool_name = tool_info.get('name', 'unknown')
						tool_desc = tool_info.get('description', 'æ— æè¿°')
						params = tool_info.get('parameters', {})
						params_desc = self._format_parameters_schema(params)
						score = tool_info.get('score', 1.0)
						# æå–å®é™…å·¥å…·åï¼ˆå»æ‰ mcp_{server}_ å‰ç¼€ï¼‰
						actual_tool_name = tool_name.split('_', 2)[-1] if '_' in tool_name else tool_name
						tool_sections.append(f"- **{actual_tool_name}** (å·¥å…·å: {tool_name}, è¯„åˆ†: {score:.2f}): {tool_desc}")
						if params_desc:
							tool_sections.append(f"  å‚æ•°: {params_desc}")
						tool_sections.append(f"  æœåŠ¡å™¨: {server_name}")
			
			# ä¸´æ—¶å·¥å…·ï¼ˆåŒæ ·æŒ‰è¯„åˆ†ä»é«˜åˆ°ä½æ’åºï¼‰
			if 'temporary' in tools_by_type:
				tool_sections.append("\n## ä¸´æ—¶å·¥å…·ï¼ˆæŒ‰è¯„åˆ†ä»é«˜åˆ°ä½æ’åºï¼Œä¼˜å…ˆé€‰æ‹©é«˜è¯„åˆ†å·¥å…·ï¼‰ï¼š")
				sorted_temp_tools = sorted(tools_by_type['temporary'], key=lambda t: t.get('score', 1.0), reverse=True)
				for tool_info in sorted_temp_tools:
					tool_name = tool_info.get('name', 'unknown')
					tool_desc = tool_info.get('description', 'æ— æè¿°')
					params = tool_info.get('parameters', {})
					params_desc = self._format_parameters_schema(params)
					score = tool_info.get('score', 1.0)
					# æå–å®é™…å·¥å…·åï¼ˆå»æ‰ temp_ å‰ç¼€ï¼‰
					actual_tool_name = tool_name.replace('temp_', '') if tool_name.startswith('temp_') else tool_name
					tool_sections.append(f"- **{actual_tool_name}** (å·¥å…·å: {tool_name}, è¯„åˆ†: {score:.2f}): {tool_desc}")
					if params_desc:
						tool_sections.append(f"  å‚æ•°: {params_desc}")
			
			result = "\n".join(tool_sections) if tool_sections else "æš‚æ— å¯ç”¨å·¥å…·"
			logger.info(f"è§„åˆ’èŠ‚ç‚¹å·¥å…·åˆ—è¡¨æ ¼å¼åŒ–å®Œæˆï¼Œé•¿åº¦: {len(result)} å­—ç¬¦")
			return result
			
		except Exception as e:
			logger.error(f"è·å–å¯ç”¨å·¥å…·åˆ—è¡¨å¤±è´¥: {str(e)}", exc_info=True)
			return f"è·å–å·¥å…·åˆ—è¡¨å¤±è´¥: {str(e)}"
	
	def _format_parameters_schema(self, params_schema: Dict[str, Any]) -> str:
		"""æ ¼å¼åŒ–å‚æ•° schema ä¸ºæ˜“è¯»çš„å­—ç¬¦ä¸²"""
		if not params_schema or not isinstance(params_schema, dict):
			return ""
		
		try:
			properties = params_schema.get('properties', {})
			required = params_schema.get('required', [])
			
			if not properties:
				return ""
			
			param_descs = []
			for param_name, param_info in properties.items():
				param_type = param_info.get('type', 'string')
				param_desc = param_info.get('description', '')
				is_required = param_name in required
				required_mark = "(å¿…å¡«)" if is_required else "(å¯é€‰)"
				
				if param_type == 'object':
					param_descs.append(f"{param_name}: å¯¹è±¡ {required_mark}")
				elif param_type == 'array':
					items = param_info.get('items', {})
					item_type = items.get('type', 'string')
					param_descs.append(f"{param_name}: {item_type}æ•°ç»„ {required_mark}")
				else:
					param_descs.append(f"{param_name}: {param_type} {required_mark}")
				if param_desc:
					param_descs[-1] += f" - {param_desc}"
			
			return ", ".join(param_descs)
		except Exception as e:
			logger.warning(f"æ ¼å¼åŒ–å‚æ•° schema å¤±è´¥: {str(e)}")
			return ""
	
	def _format_context_info(self, context: Dict[str, Any]) -> str:
		"""æ ¼å¼åŒ–ä¸Šä¸‹æ–‡ä¿¡æ¯"""
		flow_state = context.get('flow_state', {})
		if not flow_state:
			return "æ— ä¸Šä¸‹æ–‡ä¿¡æ¯"
		
		info_parts = []
		if 'last_output' in flow_state:
			info_parts.append(f"ä¸Šä¸€èŠ‚ç‚¹è¾“å‡º: {str(flow_state['last_output'])[:200]}")
		if 'saved_files' in flow_state:
			info_parts.append(f"å·²ä¿å­˜æ–‡ä»¶: {', '.join(flow_state['saved_files'])}")
		
		return "\n".join(info_parts) if info_parts else "æ— ä¸Šä¸‹æ–‡ä¿¡æ¯"
	
	async def _execute_generated_flow(
		self,
		user_id: str,
		message: str,
		context: Dict[str, Any],
		flow_config: Dict[str, Any],
		agent_name: str = None
	) -> AgentMessage:
		"""æ‰§è¡Œç”Ÿæˆçš„æµç¨‹å›¾ï¼ˆåŒæ­¥ï¼‰"""
		try:
			# åˆ›å»º FlowEngine
			engine = FlowEngine()
			engine.build_from_config(flow_config)
			
			# æ‰§è¡Œæµç¨‹å›¾
			results = await engine.run(
				user_id=user_id,
				message=message,
				context=context,
				start_node_id=None,  # ä½¿ç”¨é»˜è®¤èµ·å§‹èŠ‚ç‚¹
				agent_name=agent_name
			)
			
			# åˆå¹¶æ‰€æœ‰ç»“æœ
			content_parts = []
			for result in results:
				if result.content:
					content_parts.append(result.content)
			
			final_content = "\n\n".join(content_parts) if content_parts else "æµç¨‹å›¾æ‰§è¡Œå®Œæˆ"
			
			return self._create_agent_message(
				final_content,
				agent_name,
				metadata={
					'planner_node_id': self.id,
					'generated_flow_config': flow_config,
					'execution_results_count': len(results)
				}
			)
		except Exception as e:
			logger.error(f"è§„åˆ’èŠ‚ç‚¹ {self.id} æ‰§è¡Œç”Ÿæˆçš„æµç¨‹å›¾å¤±è´¥: {str(e)}")
			error_msg = f"æ‰§è¡Œç”Ÿæˆçš„æµç¨‹å›¾å¤±è´¥: {str(e)}"
			return self._create_agent_message(error_msg, agent_name, metadata={'error': str(e)})
	
	async def _execute_generated_nodes_stream(
		self,
		user_id: str,
		message: str,
		context: Dict[str, Any],
		generated_nodes: List[Dict[str, Any]],
		generated_edges: List[Dict[str, Any]],
		planner_next_node_id: Optional[str] = None,
		agent_name: str = None,
		retry_index: int = 0
	) -> AsyncGenerator[StreamChunk, None]:
		"""æ‰§è¡Œç”Ÿæˆçš„èŠ‚ç‚¹ï¼ˆæµå¼ï¼‰"""
		if not generated_nodes:
			logger.warning(f"è§„åˆ’èŠ‚ç‚¹ {self.id} æ²¡æœ‰ç”Ÿæˆä»»ä½•èŠ‚ç‚¹")
			return
		
		try:
			# åˆ›å»ºä¸´æ—¶ FlowEngine æ¥æ‰§è¡Œç”Ÿæˆçš„èŠ‚ç‚¹
			# æ³¨æ„ï¼šè¿™é‡Œä¸æ·»åŠ  start å’Œ end èŠ‚ç‚¹ï¼Œç›´æ¥æ‰§è¡Œç”Ÿæˆçš„èŠ‚ç‚¹
			from ..engine import FlowEngine
			engine = FlowEngine()
			
			# æ„å»ºèŠ‚ç‚¹å›¾ï¼ˆä¸è‡ªåŠ¨æ·»åŠ  start/endï¼‰
			engine._node_map.clear()
			engine._adj.clear()
			engine._in_degree.clear()
			
			# å®ä¾‹åŒ–ç”Ÿæˆçš„èŠ‚ç‚¹
			for node_cfg in generated_nodes:
				try:
					node = BaseFlowNode.from_config(node_cfg)
					engine._node_map[node.id] = node
					engine._adj[node.id] = []
					engine._in_degree[node.id] = 0
					logger.info(f"è§„åˆ’èŠ‚ç‚¹ {self.id} å®ä¾‹åŒ–ç”ŸæˆèŠ‚ç‚¹: {node.id} ({node.name})")
				except Exception as e:
					logger.error(f"è§„åˆ’èŠ‚ç‚¹ {self.id} å®ä¾‹åŒ–èŠ‚ç‚¹å¤±è´¥ {node_cfg.get('id', 'unknown')}: {str(e)}")
					continue
			
			# æ„å»ºè¾¹è¿æ¥
			for edge_cfg in generated_edges:
				source_id = edge_cfg.get('source')
				target_id = edge_cfg.get('target')
				if source_id in engine._node_map and target_id in engine._node_map:
					if target_id not in engine._adj[source_id]:
						engine._adj[source_id].append(target_id)
					engine._in_degree[target_id] = engine._in_degree.get(target_id, 0) + 1
					# æ›´æ–°èŠ‚ç‚¹çš„ connections
					source_node = engine._node_map[source_id]
					if target_id not in source_node.connections:
						source_node.add_connection(target_id)
			
			# æ‰¾åˆ°æœ€åä¸€ä¸ªèŠ‚ç‚¹ï¼ˆå‡ºåº¦ä¸º0çš„èŠ‚ç‚¹ï¼Œå³æ²¡æœ‰åç»­è¿æ¥çš„èŠ‚ç‚¹ï¼‰
			last_node_id = None
			for node_id in engine._node_map.keys():
				# æ£€æŸ¥è¯¥èŠ‚ç‚¹æ˜¯å¦æœ‰å‡ºè¾¹ï¼ˆè¿æ¥åˆ°å…¶ä»–ç”ŸæˆèŠ‚ç‚¹ï¼‰
				has_outgoing_to_generated = any(
					target_id in engine._node_map 
					for target_id in engine._adj.get(node_id, [])
				)
				if not has_outgoing_to_generated:
					last_node_id = node_id
					break
			
			# å¦‚æœæ²¡æœ‰æ‰¾åˆ°å‡ºåº¦ä¸º0çš„èŠ‚ç‚¹ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªèŠ‚ç‚¹
			if not last_node_id and engine._node_map:
				# æ‰¾åˆ°æ‰§è¡Œé¡ºåºä¸­çš„æœ€åä¸€ä¸ªèŠ‚ç‚¹
				start_nodes = [node_id for node_id, in_deg in engine._in_degree.items() if in_deg == 0]
				if start_nodes:
					current = start_nodes[0]
					while True:
						next_id = engine._node_map[current].get_next_node_id(0)
						if next_id and next_id in engine._node_map:
							current = next_id
						else:
							last_node_id = current
							break
				else:
					last_node_id = list(engine._node_map.keys())[-1]
			
			# å°†æœ€åä¸€ä¸ªç”ŸæˆèŠ‚ç‚¹è¿æ¥åˆ°è§„åˆ’èŠ‚ç‚¹çš„åŸå§‹ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
			# æ³¨æ„ï¼šè¿™é‡Œä»…ç”¨äºé¦–è½®è§„åˆ’ï¼Œé‡æ–°è§„åˆ’æ—¶æˆ‘ä»¬ä¸å†ä»æ–°è·¯çº¿è¿å›åŸæ¥çš„ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ï¼Œ
			# ä»¥ä¿è¯æ–°æ—§ä¸¤æ¡è·¯çº¿åœ¨å›¾ç»“æ„ä¸Šå®Œå…¨ç‹¬ç«‹ã€‚
			if last_node_id and planner_next_node_id and retry_index == 0:
				last_node = engine._node_map.get(last_node_id)
				if last_node:
					last_node.add_connection(planner_next_node_id)
					logger.info(f"è§„åˆ’èŠ‚ç‚¹ {self.id} å°†æœ€åä¸€ä¸ªç”ŸæˆèŠ‚ç‚¹ {last_node_id} è¿æ¥åˆ°è§„åˆ’èŠ‚ç‚¹çš„ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ {planner_next_node_id}")
			
			# æ‰¾åˆ°èµ·å§‹èŠ‚ç‚¹ï¼ˆå…¥åº¦ä¸º0çš„èŠ‚ç‚¹ï¼‰
			start_nodes = [node_id for node_id, in_deg in engine._in_degree.items() if in_deg == 0]
			if not start_nodes:
				# å¦‚æœæ²¡æœ‰å…¥åº¦ä¸º0çš„èŠ‚ç‚¹ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªèŠ‚ç‚¹
				start_nodes = [list(engine._node_map.keys())[0]] if engine._node_map else []
			
			if not start_nodes:
				logger.warning(f"è§„åˆ’èŠ‚ç‚¹ {self.id} æ²¡æœ‰æ‰¾åˆ°èµ·å§‹èŠ‚ç‚¹")
				return
			
			# æ‰§è¡Œç”Ÿæˆçš„èŠ‚ç‚¹ï¼ˆä»ç¬¬ä¸€ä¸ªèµ·å§‹èŠ‚ç‚¹å¼€å§‹ï¼‰
			current_node_id = start_nodes[0]
			executed_nodes = set()
			failed_nodes: List[Dict[str, str]] = []  # æ”¶é›†å¤±è´¥èŠ‚ç‚¹ä¿¡æ¯
			
			while current_node_id and current_node_id not in executed_nodes:
				executed_nodes.add(current_node_id)
				node = engine._node_map.get(current_node_id)
				if not node:
					logger.warning(f"è§„åˆ’èŠ‚ç‚¹ {self.id} èŠ‚ç‚¹ä¸å­˜åœ¨: {current_node_id}")
					break
				
				# æ‰§è¡ŒèŠ‚ç‚¹ï¼ˆæµå¼ï¼‰
				node_failed = False
				node_error = None
				node_start_sent = False
				node_complete_sent = False
				node_label = node.config.get('label') if hasattr(node, 'config') else None
				node_metadata = {
					'node_id': node.id,
					'node_type': getattr(getattr(node, 'type', None), 'value', getattr(node, 'type', 'unknown')),
					'node_name': getattr(node, 'name', node.id),
					'node_label': node_label or getattr(node, 'name', node.id)
				}
				node_output_chunks: List[str] = []
				
				def emit_node_start_chunk():
					nonlocal node_start_sent
					if node_start_sent:
						return None
					node_start_sent = True
					return node._create_stream_chunk(
						chunk_type="node_start",
						content=f"ğŸš€ å¼€å§‹æ‰§è¡Œ {getattr(node, 'name', node.id)} èŠ‚ç‚¹",
						agent_name=agent_name,
						metadata=node_metadata.copy()
					)
				
				def emit_node_complete_chunk(status: str, output: Optional[str] = None, error: Optional[str] = None):
					nonlocal node_complete_sent
					if node_complete_sent:
						return None
					node_complete_sent = True
					metadata = node_metadata.copy()
					metadata['status'] = status
					if output:
						metadata['output'] = output
					if error:
						metadata['error'] = error
					return node._create_stream_chunk(
						chunk_type="node_complete",
						content=f"{'âœ…' if status == 'completed' else 'âš ï¸'} {getattr(node, 'name', node.id)} èŠ‚ç‚¹æ‰§è¡Œ{ 'å®Œæˆ' if status == 'completed' else 'ç»“æŸ'}",
						agent_name=agent_name,
						metadata=metadata
					)
				
				try:
					start_chunk = emit_node_start_chunk()
					if start_chunk:
						yield start_chunk
					
					async for chunk in node.execute_stream(user_id, message, context, agent_name):
						if chunk.type == "node_start":
							node_start_sent = True
						if chunk.type == "node_complete":
							node_complete_sent = True
						if chunk.type in ("content", "final_response", "final") and isinstance(chunk.content, str):
							node_output_chunks.append(chunk.content)
						
						# æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯äº‹ä»¶
						if chunk.type == "node_error":
							node_failed = True
							node_error = chunk.content or chunk.metadata.get('error', 'èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥')
							logger.warning(f"è§„åˆ’èŠ‚ç‚¹ {self.id} æ£€æµ‹åˆ°èŠ‚ç‚¹ {current_node_id} æ‰§è¡Œå¤±è´¥: {node_error}")
						elif chunk.type == "node_complete" and chunk.metadata:
							# æ£€æŸ¥èŠ‚ç‚¹å®Œæˆäº‹ä»¶ä¸­æ˜¯å¦æ ‡è®°ä¸ºå¤±è´¥
							if chunk.metadata.get('status') == 'failed' or chunk.metadata.get('error'):
								node_failed = True
								node_error = chunk.metadata.get('error', chunk.metadata.get('output', 'èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥'))
								logger.warning(f"è§„åˆ’èŠ‚ç‚¹ {self.id} æ£€æµ‹åˆ°èŠ‚ç‚¹ {current_node_id} æ‰§è¡Œå¤±è´¥: {node_error}")
						
						# é€ä¼ èŠ‚ç‚¹çš„æµå¼è¾“å‡º
						yield chunk
				except Exception as e:
					node_failed = True
					node_error = str(e)
					logger.error(f"è§„åˆ’èŠ‚ç‚¹ {self.id} æ‰§è¡ŒèŠ‚ç‚¹ {current_node_id} å¤±è´¥: {str(e)}")
					yield self._create_stream_chunk(
						chunk_type="content",
						content=f"âŒ èŠ‚ç‚¹ {node.name} æ‰§è¡Œå¤±è´¥: {str(e)}\n",
						agent_name=agent_name
					)
				
				if not node_failed:
					complete_chunk = emit_node_complete_chunk(
						status="completed",
						output=("".join(node_output_chunks)).strip() if node_output_chunks else None
					)
					if complete_chunk:
						yield complete_chunk
				
				# å¦‚æœèŠ‚ç‚¹å¤±è´¥ï¼Œè®°å½•é”™è¯¯ä¿¡æ¯å¹¶ç«‹å³åœæ­¢æµç¨‹
				if node_failed:
					failed_nodes.append({
						'node_id': current_node_id,
						'node_name': node.name,
						'error': node_error or 'èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥'
					})
					failed_chunk = emit_node_complete_chunk(
						status="failed",
						output=("".join(node_output_chunks)).strip() if node_output_chunks else None,
						error=node_error
					)
					if failed_chunk:
						yield failed_chunk
					
					# ä¸€æ—¦æ£€æµ‹åˆ°å¤±è´¥ï¼Œç«‹å³åœæ­¢æ‰§è¡Œåç»­èŠ‚ç‚¹
					logger.warning(f"è§„åˆ’èŠ‚ç‚¹ {self.id} æ£€æµ‹åˆ°èŠ‚ç‚¹ {current_node_id} å¤±è´¥ï¼Œç«‹å³åœæ­¢æµç¨‹æ‰§è¡Œ")
					current_node_id = None  # åœæ­¢æ‰§è¡Œ
					break  # è·³å‡ºå¾ªç¯ï¼Œä¸å†æ‰§è¡Œåç»­èŠ‚ç‚¹
				
				# é€‰æ‹©ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
				next_node_id = node.get_next_node_id(0)
				# å¦‚æœä¸‹ä¸€ä¸ªèŠ‚ç‚¹æ˜¯è§„åˆ’èŠ‚ç‚¹çš„åŸå§‹ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ï¼Œç»“æŸæ‰§è¡Œï¼ˆè®© FlowEngine ç»§ç»­æ‰§è¡Œï¼‰
				# å¯¹äºé‡è¯•åœºæ™¯ï¼ˆretry_index > 0ï¼‰ï¼Œæ–°å­æµç¨‹ä¸å†è¿å›åŸè·¯çº¿ï¼Œç›´æ¥åœ¨æœ¬å­æµç¨‹å†…ç»ˆæ­¢ã€‚
				if retry_index == 0 and next_node_id == planner_next_node_id:
					logger.info(f"è§„åˆ’èŠ‚ç‚¹ {self.id} ç”Ÿæˆçš„èŠ‚ç‚¹æ‰§è¡Œå®Œæˆï¼Œå°†ç»§ç»­æ‰§è¡Œè§„åˆ’èŠ‚ç‚¹çš„ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ {planner_next_node_id}")
					current_node_id = None
				elif next_node_id and next_node_id in engine._node_map:
					current_node_id = next_node_id
				else:
					# æ²¡æœ‰ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ï¼Œç»“æŸ
					current_node_id = None
			
			# é¦–æ¬¡è§„åˆ’ä¸”å…¨ç¨‹æ— å¤±è´¥æ—¶ï¼Œå°†å­æµç¨‹æœ€åä¸€ä¸ªèŠ‚ç‚¹è¿åˆ°å…¨å±€å”¯ä¸€çš„ç»“æŸèŠ‚ç‚¹ end_nodeï¼Œ
			# è¿™æ ·æ‰€æœ‰ä¸åŒçš„è·¯çº¿ï¼ˆåˆå§‹è·¯çº¿ + å„æ¬¡é‡è¯•ï¼‰æœ€ç»ˆéƒ½ä¼šåœ¨å‰ç«¯æ±‡èšåˆ°åŒä¸€ä¸ªç»“æŸèŠ‚ç‚¹ã€‚
			if not failed_nodes and last_node_id and retry_index == 0:
				global_end_id = "end_node"
				end_edge = {
					'id': f"edge_{last_node_id}_{global_end_id}",
					'source': last_node_id,
					'target': global_end_id,
					'type': 'default'
				}
				logger.info(f"è§„åˆ’èŠ‚ç‚¹ {self.id} é¦–æ¬¡è§„åˆ’æˆåŠŸï¼Œè¿æ¥ {last_node_id} -> {global_end_id} ä½œä¸ºç»Ÿä¸€ç»“æŸèŠ‚ç‚¹")
				yield self._create_stream_chunk(
					chunk_type="flow_nodes_extend",
					content="",
					agent_name=agent_name,
					metadata={
						'planner_node_id': self.id,
						'planner_next_node_id': planner_next_node_id,
						'remove_planner_edge': False,
						'nodes': [],
						'edges': [end_edge],
						'flow_name': 'è¿æ¥åˆ°å…¨å±€ç»“æŸèŠ‚ç‚¹',
						'node_count': 0,
						'is_virtual_end': False
					}
				)
			
			# å¦‚æœæ£€æµ‹åˆ°å¤±è´¥èŠ‚ç‚¹ï¼Œç«‹å³åœæ­¢æµç¨‹ï¼Œå¹¶åœ¨è§„åˆ’èŠ‚ç‚¹ä¸‹æ–°å¢â€œé‡æ–°è§„åˆ’â€å­èŠ‚ç‚¹æŒ‚è½½æ–°å­æµç¨‹
			if failed_nodes:
				logger.warning(f"è§„åˆ’èŠ‚ç‚¹ {self.id} æ£€æµ‹åˆ° {len(failed_nodes)} ä¸ªå¤±è´¥èŠ‚ç‚¹ï¼Œåœæ­¢å½“å‰æµç¨‹ï¼Œé‡æ–°è§„åˆ’æ–°çº¿è·¯")

				# è®¡ç®—æœ¬æ¬¡é‡è¯•çš„è™šæ‹Ÿè§„åˆ’èŠ‚ç‚¹IDå’Œæ ‡ç­¾ï¼ˆç”¨äºå‰ç«¯å’Œå·¦ä¾§èŠå¤©èŠ‚ç‚¹ï¼‰
				next_retry_index = retry_index + 1
				retry_planner_node_id = f"{self.id}_retry_{next_retry_index}"
				retry_label = "é‡æ–°è§„åˆ’" if next_retry_index == 1 else f"é‡æ–°è§„åˆ’ {next_retry_index} æ¬¡"

				# ä¸ºâ€œé‡æ–°è§„åˆ’â€åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„èŠ‚ç‚¹ï¼ˆå·¦ä¾§èŠå¤©ä¸­çš„æ–°èŠ‚ç‚¹ï¼‰
				yield self._create_stream_chunk(
					chunk_type="node_start",
					content=f"ğŸ” {retry_label}ï¼šå‡†å¤‡é‡æ–°è§„åˆ’æ–°çš„å­æµç¨‹...\n",
					agent_name=agent_name,
					metadata={
						"node_id": retry_planner_node_id,
						"node_type": "planner_retry",
						"node_name": self.name,
						"node_label": retry_label,
					},
				)

				# å°†å¤±è´¥è¯´æ˜ä¹Ÿå½’å…¥è¿™ä¸ªâ€œé‡æ–°è§„åˆ’â€èŠ‚ç‚¹
				yield self._create_stream_chunk(
					chunk_type="content",
					content=f"\nâš ï¸ æ£€æµ‹åˆ°èŠ‚ç‚¹æ‰§è¡Œå¤±è´¥ï¼Œå·²åœæ­¢å½“å‰æµç¨‹ï¼Œæ­£åœ¨é‡æ–°è§„åˆ’æ–°çº¿è·¯...\n\n",
					agent_name=agent_name,
					metadata={
						"node_id": retry_planner_node_id,
						"node_type": "planner_retry",
						"node_name": self.name,
						"node_label": retry_label,
					},
				)
				
				# æ”¶é›†é”™è¯¯ä¿¡æ¯
				error_summary = self._format_failed_nodes_summary(failed_nodes)
				
				# é‡æ–°ç”Ÿæˆæµç¨‹å›¾é…ç½®ï¼ˆåŒ…å«é”™è¯¯ä¿¡æ¯ï¼‰
				retry_flow_config = await self._generate_flow_config_with_errors(
					message, context, error_summary, next_retry_index
				)
				
				if retry_flow_config:
					flow_name = retry_flow_config.get('metadata', {}).get('name', 'é‡æ–°è§„åˆ’çš„æµç¨‹å›¾')
					retry_nodes = retry_flow_config.get('nodes', [])
					node_count = len(retry_nodes)

					# å°†â€œé‡æ–°ç”Ÿæˆæ–°çº¿è·¯â€çš„è¯´æ˜æ–‡æœ¬ä¹Ÿå½’å…¥é‡æ–°è§„åˆ’èŠ‚ç‚¹
					yield self._create_stream_chunk(
						chunk_type="content",
						content=f"âœ… å·²é‡æ–°ç”Ÿæˆ {node_count} ä¸ªèŠ‚ç‚¹çš„æ–°çº¿è·¯ï¼š{flow_name}\n\n",
						agent_name=agent_name,
						metadata={
							"node_id": retry_planner_node_id,
							"node_type": "planner_retry",
							"node_name": self.name,
							"node_label": retry_label,
						},
					)

					# æ‰¾åˆ°æœ€åä¸€ä¸ªç”ŸæˆèŠ‚ç‚¹IDï¼ˆç”¨äºè¿æ¥è™šæ‹Ÿç»“æŸèŠ‚ç‚¹ï¼‰
					last_retry_node_id = self._find_last_node_id(retry_nodes, retry_flow_config.get('edges', []))
					
					# ç”Ÿæˆâ€œé‡æ–°è§„åˆ’èŠ‚ç‚¹ + æ–°å­æµç¨‹â€å±•ç¤ºç»“æ„
					display_retry_nodes, display_retry_edges, retry_planner_node_id = self._build_retry_flow_display_nodes(
						self.id,
						next_retry_index,
						retry_nodes,
						retry_flow_config.get('edges', []),
						last_retry_node_id
					)
					
					yield self._create_stream_chunk(
						chunk_type="flow_nodes_extend",
						content="",
						agent_name=agent_name,
						metadata={
							'planner_node_id': self.id,                 # åŸå§‹è§„åˆ’èŠ‚ç‚¹IDï¼ˆç”¨äºä»åŸè§„åˆ’èŠ‚ç‚¹è¿åˆ° retry èŠ‚ç‚¹ï¼‰
							'planner_next_node_id': planner_next_node_id,
							'remove_planner_edge': False,               # ä¸ç§»é™¤åŸæœ‰è¿æ¥ï¼Œä¿ç•™å¤±è´¥è·¯å¾„
							'replace_existing_nodes': False,           # è¿½åŠ æ¨¡å¼ï¼Œä¿ç•™æ—§èŠ‚ç‚¹
							'nodes': display_retry_nodes,
							'edges': display_retry_edges,
							'flow_name': flow_name,
							'node_count': len(display_retry_nodes),
							'is_retry': True,                          # æ ‡è®°ä¸ºé‡æ–°è§„åˆ’
							'root_planner_node_id': self.id,
							'retry_planner_node_id': retry_planner_node_id,
							'retry_index': next_retry_index
						}
					)

					# æ ‡è®°â€œé‡æ–°è§„åˆ’â€èŠ‚ç‚¹å®Œæˆï¼Œè®©å·¦ä¾§èŠå¤©ä¸­çš„è¯¥èŠ‚ç‚¹çŠ¶æ€ä¸ºå·²å®Œæˆ
					retry_output_summary = f"{retry_label}ï¼šå·²é‡æ–°ç”Ÿæˆ {node_count} ä¸ªèŠ‚ç‚¹çš„æ–°çº¿è·¯ï¼š{flow_name}"
					yield self._create_stream_chunk(
						chunk_type="node_complete",
						content=f"âœ… {retry_label} å®Œæˆï¼Œå…±ç”Ÿæˆ {node_count} ä¸ªèŠ‚ç‚¹çš„æ–°å­æµç¨‹",
						agent_name=agent_name,
						metadata={
							"node_id": retry_planner_node_id,
							"node_type": "planner_retry",
							"node_name": self.name,
							"node_label": retry_label,
							"status": "completed",
							"output": retry_output_summary,
						},
					)
					
					# æ‰§è¡Œé‡æ–°è§„åˆ’çš„èŠ‚ç‚¹ï¼ˆé€’å½’è°ƒç”¨ï¼Œæ”¯æŒå¤šæ¬¡é‡è¯•ï¼‰
					async for chunk in self._execute_generated_nodes_stream(
						user_id, message, context, retry_nodes, 
						retry_flow_config.get('edges', []), planner_next_node_id, agent_name, next_retry_index
					):
						yield chunk
				else:
					yield self._create_stream_chunk(
						chunk_type="content",
						content=f"âŒ é‡æ–°è§„åˆ’å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆæ–°çš„æµç¨‹å›¾é…ç½®\n",
						agent_name=agent_name
					)
					
		except Exception as e:
			logger.error(f"è§„åˆ’èŠ‚ç‚¹ {self.id} æµå¼æ‰§è¡Œç”Ÿæˆçš„èŠ‚ç‚¹å¤±è´¥: {str(e)}")
			error_msg = f"æ‰§è¡Œç”Ÿæˆçš„èŠ‚ç‚¹å¤±è´¥: {str(e)}"
			yield self._create_stream_chunk(
				chunk_type="content",
				content=f"âŒ {error_msg}\n",
				agent_name=agent_name,
				is_end=True
			)

