# çŸ¥è¯†å›¾è°±ä¸‰å…ƒç»„æŠ½å– - åˆ†æ­¥å¼‚æ­¥å¤„ç†æ–¹æ¡ˆè®¾è®¡

## ä¸€ã€è®¾è®¡ç›®æ ‡

1. **åˆ†æ­¥å¤„ç†**ï¼šå…ˆå®Œæˆæ–‡æ¡£åˆ†å—å’Œå‘é‡åŒ–ï¼ˆå¿«é€Ÿè¿”å›ï¼‰ï¼Œä¸‰å…ƒç»„æŠ½å–åœ¨åå°å¼‚æ­¥è¿›è¡Œ
2. **çŠ¶æ€è¿½è¸ª**ï¼šå‰ç«¯å¯ä»¥å®æ—¶æŸ¥çœ‹å¤„ç†è¿›åº¦ï¼ˆåˆ†å—å®Œæˆã€ä¸‰å…ƒç»„æŠ½å–è¿›åº¦ï¼‰
3. **æ¨¡å‹é›†æˆ**ï¼šä½¿ç”¨ç»„åˆ1ï¼ˆRoBERTa-CLUE NER + CasRel REï¼‰æ›¿ä»£LLMæŠ½å–ï¼Œæå‡é€Ÿåº¦å’Œç¨³å®šæ€§
4. **ç”¨æˆ·ä½“éªŒ**ï¼šæ–‡æ¡£ä¸Šä¼ åç«‹å³æ˜¾ç¤ºåˆ†å—å®Œæˆï¼Œåå°æ˜¾ç¤ºä¸‰å…ƒç»„æŠ½å–è¿›åº¦

## äºŒã€æ¶æ„è®¾è®¡

### 2.1 å¤„ç†æµç¨‹

```
æ–‡æ¡£ä¸Šä¼ 
  â†“
[åŒæ­¥é˜¶æ®µ] åˆ†å— + å‘é‡åŒ–
  â”œâ”€ æ›´æ–°æ–‡æ¡£çŠ¶æ€: processing â†’ chunking â†’ chunked
  â”œâ”€ åˆ›å»ºæ‰€æœ‰ DocumentChunk è®°å½•
  â””â”€ ç”Ÿæˆå¹¶å­˜å‚¨å‘é‡åµŒå…¥
  â†“
[å¼‚æ­¥é˜¶æ®µ] ä¸‰å…ƒç»„æŠ½å–ï¼ˆåå°çº¿ç¨‹ï¼‰
  â”œâ”€ ä½¿ç”¨ NER + RE æ¨¡å‹æŠ½å–ä¸‰å…ƒç»„
  â”œâ”€ æ›´æ–°æ¯ä¸ªåˆ†å—çš„ kg_extraction_status
  â””â”€ æ›´æ–°æ–‡æ¡£çš„ kg_extraction_status
```

### 2.2 çŠ¶æ€å®šä¹‰

#### Document çŠ¶æ€æ‰©å±•
- `status`: `pending` â†’ `processing` â†’ `chunking` â†’ `chunked` â†’ `completed`
- `kg_extraction_status`: `pending` | `processing` | `completed` | `failed` | `skipped`
- `kg_extraction_progress`: `{"total_chunks": 10, "processed": 5, "failed": 0}`

#### DocumentChunk çŠ¶æ€
- `kg_extraction_status`: `pending` | `processing` | `completed` | `failed` | `skipped`
- `kg_triples_count`: è¯¥åˆ†å—æŠ½å–åˆ°çš„ä¸‰å…ƒç»„æ•°é‡

## ä¸‰ã€æ•°æ®åº“æ¨¡å‹ä¿®æ”¹

### 3.1 Document è¡¨æ–°å¢å­—æ®µ

```python
# agent-backend/models/database_models.py

class Document(Base):
    # ... ç°æœ‰å­—æ®µ ...
    
    # æ–°å¢ï¼šçŸ¥è¯†å›¾è°±æŠ½å–çŠ¶æ€
    kg_extraction_status = Column(String(50), default="pending")  # pending, processing, completed, failed, skipped
    kg_extraction_progress = Column(JSON, nullable=True)  # {"total_chunks": 10, "processed": 5, "failed": 0}
    kg_extraction_started_at = Column(DateTime, nullable=True)
    kg_extraction_completed_at = Column(DateTime, nullable=True)
```

### 3.2 DocumentChunk è¡¨æ–°å¢å­—æ®µ

```python
# agent-backend/models/database_models.py

class DocumentChunk(Base):
    # ... ç°æœ‰å­—æ®µ ...
    
    # æ–°å¢ï¼šçŸ¥è¯†å›¾è°±æŠ½å–çŠ¶æ€
    kg_extraction_status = Column(String(50), default="pending")  # pending, processing, completed, failed, skipped
    kg_triples_count = Column(Integer, default=0)  # è¯¥åˆ†å—æŠ½å–åˆ°çš„ä¸‰å…ƒç»„æ•°é‡
    kg_extraction_error = Column(Text, nullable=True)  # æŠ½å–å¤±è´¥æ—¶çš„é”™è¯¯ä¿¡æ¯
```

## å››ã€åå°ä»»åŠ¡å¤„ç†

### 4.1 ä»»åŠ¡é˜Ÿåˆ—è®¾è®¡

ä½¿ç”¨ `concurrent.futures.ThreadPoolExecutor` åˆ›å»ºåå°ä»»åŠ¡æ± ï¼š

```python
# agent-backend/services/kg_extraction_worker.py (æ–°å»º)

import threading
from concurrent.futures import ThreadPoolExecutor, Future
from typing import Dict, Optional
from sqlalchemy.orm import Session
from database.database import get_db_session
from models.database_models import Document, DocumentChunk
from services.knowledge_graph_service import KnowledgeGraphService
from utils.log_helper import get_logger

logger = get_logger("kg_extraction_worker")

class KGExtractionWorker:
    """çŸ¥è¯†å›¾è°±ä¸‰å…ƒç»„æŠ½å–åå°å·¥ä½œå™¨"""
    
    def __init__(self, max_workers: int = 2):
        self.executor = ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="kg_extractor")
        self.running_tasks: Dict[int, Future] = {}  # doc_id -> Future
        self.kg_service = KnowledgeGraphService()
    
    def submit_document_extraction(self, doc_id: int):
        """æäº¤æ–‡æ¡£çš„ä¸‰å…ƒç»„æŠ½å–ä»»åŠ¡"""
        if doc_id in self.running_tasks:
            logger.warning(f"æ–‡æ¡£ {doc_id} çš„ä¸‰å…ƒç»„æŠ½å–ä»»åŠ¡å·²åœ¨è¿è¡Œä¸­")
            return
        
        future = self.executor.submit(self._extract_triples_for_document, doc_id)
        self.running_tasks[doc_id] = future
        
        # ä»»åŠ¡å®Œæˆæ—¶æ¸…ç†
        def cleanup(fut):
            if doc_id in self.running_tasks:
                del self.running_tasks[doc_id]
        future.add_done_callback(cleanup)
        
        logger.info(f"å·²æäº¤æ–‡æ¡£ {doc_id} çš„ä¸‰å…ƒç»„æŠ½å–ä»»åŠ¡")
    
    def _extract_triples_for_document(self, doc_id: int):
        """ä¸ºæ–‡æ¡£çš„æ‰€æœ‰åˆ†å—æŠ½å–ä¸‰å…ƒç»„"""
        db: Optional[Session] = None
        try:
            db = get_db_session()
            doc = db.query(Document).filter(Document.id == doc_id).first()
            if not doc:
                logger.error(f"æ–‡æ¡£ {doc_id} ä¸å­˜åœ¨")
                return
            
            # æ›´æ–°æ–‡æ¡£çŠ¶æ€
            doc.kg_extraction_status = "processing"
            doc.kg_extraction_started_at = datetime.utcnow()
            db.commit()
            
            # è·å–æ‰€æœ‰å¾…å¤„ç†çš„åˆ†å—
            chunks = db.query(DocumentChunk).filter(
                DocumentChunk.document_id == doc_id,
                DocumentChunk.kg_extraction_status == "pending"
            ).order_by(DocumentChunk.chunk_index).all()
            
            if not chunks:
                logger.info(f"æ–‡æ¡£ {doc_id} æ²¡æœ‰å¾…å¤„ç†çš„åˆ†å—")
                doc.kg_extraction_status = "skipped"
                db.commit()
                return
            
            total_chunks = len(chunks)
            processed = 0
            failed = 0
            
            # åˆå§‹åŒ–è¿›åº¦
            doc.kg_extraction_progress = {
                "total_chunks": total_chunks,
                "processed": 0,
                "failed": 0
            }
            db.commit()
            
            logger.info(f"å¼€å§‹ä¸ºæ–‡æ¡£ {doc_id} çš„ {total_chunks} ä¸ªåˆ†å—æŠ½å–ä¸‰å…ƒç»„")
            
            # é€ä¸ªå¤„ç†åˆ†å—
            for chunk in chunks:
                try:
                    # æ›´æ–°åˆ†å—çŠ¶æ€
                    chunk.kg_extraction_status = "processing"
                    db.commit()
                    
                    # ä½¿ç”¨çŸ¥è¯†å›¾è°±æœåŠ¡æŠ½å–ä¸‰å…ƒç»„
                    triples_data = self.kg_service.extract_entities_and_relations(
                        text=chunk.content,
                        kb_id=doc.knowledge_base_id,
                        doc_id=doc.id,
                        chunk_id=chunk.id
                    )
                    
                    if triples_data:
                        # å­˜å‚¨ä¸‰å…ƒç»„
                        stored_count = self.kg_service.store_triples(db, triples_data)
                        chunk.kg_triples_count = stored_count
                        chunk.kg_extraction_status = "completed"
                        logger.info(f"åˆ†å— {chunk.id} æˆåŠŸæŠ½å– {stored_count} ä¸ªä¸‰å…ƒç»„")
                    else:
                        chunk.kg_extraction_status = "completed"  # æ²¡æœ‰ä¸‰å…ƒç»„ä¹Ÿç®—å®Œæˆ
                        chunk.kg_triples_count = 0
                        logger.debug(f"åˆ†å— {chunk.id} æœªæŠ½å–åˆ°ä¸‰å…ƒç»„")
                    
                    processed += 1
                    
                except Exception as e:
                    logger.error(f"åˆ†å— {chunk.id} ä¸‰å…ƒç»„æŠ½å–å¤±è´¥: {str(e)}", exc_info=True)
                    chunk.kg_extraction_status = "failed"
                    chunk.kg_extraction_error = str(e)
                    failed += 1
                
                # æ›´æ–°è¿›åº¦
                doc.kg_extraction_progress = {
                    "total_chunks": total_chunks,
                    "processed": processed,
                    "failed": failed
                }
                db.commit()
            
            # æ›´æ–°æ–‡æ¡£æœ€ç»ˆçŠ¶æ€
            if failed == 0:
                doc.kg_extraction_status = "completed"
            elif processed > 0:
                doc.kg_extraction_status = "completed"  # éƒ¨åˆ†æˆåŠŸä¹Ÿç®—å®Œæˆ
            else:
                doc.kg_extraction_status = "failed"
            
            doc.kg_extraction_completed_at = datetime.utcnow()
            db.commit()
            
            logger.info(f"æ–‡æ¡£ {doc_id} ä¸‰å…ƒç»„æŠ½å–å®Œæˆ: æˆåŠŸ {processed}, å¤±è´¥ {failed}")
            
        except Exception as e:
            logger.error(f"æ–‡æ¡£ {doc_id} ä¸‰å…ƒç»„æŠ½å–ä»»åŠ¡å¤±è´¥: {str(e)}", exc_info=True)
            if db:
                try:
                    doc = db.query(Document).filter(Document.id == doc_id).first()
                    if doc:
                        doc.kg_extraction_status = "failed"
                        db.commit()
                except:
                    pass
        finally:
            if db:
                db.close()
    
    def is_document_processing(self, doc_id: int) -> bool:
        """æ£€æŸ¥æ–‡æ¡£æ˜¯å¦æ­£åœ¨å¤„ç†ä¸­"""
        return doc_id in self.running_tasks
    
    def shutdown(self, wait: bool = True):
        """å…³é—­å·¥ä½œå™¨"""
        self.executor.shutdown(wait=wait)

# å…¨å±€å•ä¾‹
_kg_worker: Optional[KGExtractionWorker] = None

def get_kg_worker() -> KGExtractionWorker:
    """è·å–å…¨å±€KGæŠ½å–å·¥ä½œå™¨"""
    global _kg_worker
    if _kg_worker is None:
        max_workers = int(os.getenv("KG_EXTRACTION_WORKERS", "2"))
        _kg_worker = KGExtractionWorker(max_workers=max_workers)
    return _kg_worker
```

### 4.2 é›†æˆåˆ°çŸ¥è¯†åº“æœåŠ¡

ä¿®æ”¹ `knowledge_base_service.py` çš„ `_process_document_sync` æ–¹æ³•ï¼š

```python
# agent-backend/services/knowledge_base_service.py

def _process_document_sync(self, db: Session, doc: Document):
    """åŒæ­¥å¤„ç†æ–‡æ¡£ï¼ˆåˆ†å—å’Œå‘é‡åŒ–ï¼Œä¸‰å…ƒç»„æŠ½å–å¼‚æ­¥è¿›è¡Œï¼‰"""
    try:
        logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {doc.name}")
        
        # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
        doc.status = "processing"
        doc.kg_extraction_status = "pending"  # åˆå§‹åŒ–KGæŠ½å–çŠ¶æ€
        db.commit()
        
        # ... åˆ†å—å¤„ç†é€»è¾‘ï¼ˆä¿æŒä¸å˜ï¼‰ ...
        
        # åˆ›å»ºåˆ†å—è®°å½•ï¼ˆç§»é™¤ä¸‰å…ƒç»„æŠ½å–é€»è¾‘ï¼‰
        created_chunks: List[DocumentChunk] = []
        for i, (chunk_content, embedding, metadata) in enumerate(zip(chunk_contents, embeddings, chunk_metadata_list)):
            chunk = DocumentChunk(
                knowledge_base_id=doc.knowledge_base_id,
                document_id=doc.id,
                chunk_index=i,
                content=chunk_content,
                embedding=embedding,
                chunk_metadata=metadata,
                chunk_strategy=CHUNK_STRATEGY,
                strategy_variant=strategy_variant,
                kg_extraction_status="pending"  # åˆå§‹çŠ¶æ€
            )
            db.add(chunk)
            created_chunks.append(chunk)
        
        db.commit()
        
        # æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºåˆ†å—å®Œæˆ
        doc.status = "chunked"  # æ–°å¢çŠ¶æ€ï¼šåˆ†å—å®Œæˆä½†ä¸‰å…ƒç»„æœªæŠ½å–
        doc.document_metadata = doc.document_metadata or {}
        doc.document_metadata["chunk_count"] = len(chunks)
        doc.document_metadata["processing_time"] = datetime.utcnow().isoformat()
        db.commit()
        
        # æäº¤åå°ä»»åŠ¡ï¼šå¼‚æ­¥æŠ½å–ä¸‰å…ƒç»„
        if EXTRACT_TRIPLES_ENABLED:
            from services.kg_extraction_worker import get_kg_worker
            kg_worker = get_kg_worker()
            kg_worker.submit_document_extraction(doc.id)
            logger.info(f"å·²æäº¤æ–‡æ¡£ {doc.id} çš„ä¸‰å…ƒç»„æŠ½å–åå°ä»»åŠ¡")
        
        # ... å…¶ä»–å¤„ç†é€»è¾‘ï¼ˆé¢†åŸŸè¯†åˆ«ã€æ‘˜è¦ç”Ÿæˆç­‰ï¼‰ ...
        
    except Exception as e:
        logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥: {str(e)}", exc_info=True)
        doc.status = "failed"
        db.commit()
```

## äº”ã€API æ¥å£è®¾è®¡

### 5.1 æŸ¥è¯¢æ–‡æ¡£å¤„ç†çŠ¶æ€

```python
# agent-backend/api/knowledge_base.py

@router.get("/{kb_id}/documents/{doc_id}/status")
async def get_document_status(
    kb_id: int,
    doc_id: int,
    db: Session = Depends(get_db)
):
    """è·å–æ–‡æ¡£å¤„ç†çŠ¶æ€ï¼ˆåŒ…æ‹¬åˆ†å—å’Œä¸‰å…ƒç»„æŠ½å–è¿›åº¦ï¼‰"""
    try:
        doc = db.query(Document).filter(
            Document.id == doc_id,
            Document.knowledge_base_id == kb_id
        ).first()
        
        if not doc:
            raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
        
        # ç»Ÿè®¡åˆ†å—çš„ä¸‰å…ƒç»„æŠ½å–çŠ¶æ€
        chunks = db.query(DocumentChunk).filter(
            DocumentChunk.document_id == doc_id
        ).all()
        
        chunk_stats = {
            "total": len(chunks),
            "pending": sum(1 for c in chunks if c.kg_extraction_status == "pending"),
            "processing": sum(1 for c in chunks if c.kg_extraction_status == "processing"),
            "completed": sum(1 for c in chunks if c.kg_extraction_status == "completed"),
            "failed": sum(1 for c in chunks if c.kg_extraction_status == "failed"),
        }
        
        return {
            "document_id": doc.id,
            "status": doc.status,  # pending, processing, chunking, chunked, completed, failed
            "kg_extraction_status": doc.kg_extraction_status,  # pending, processing, completed, failed, skipped
            "kg_extraction_progress": doc.kg_extraction_progress or {},
            "chunk_stats": chunk_stats,
            "total_triples": sum(c.kg_triples_count or 0 for c in chunks),
            "kg_extraction_started_at": doc.kg_extraction_started_at.isoformat() if doc.kg_extraction_started_at else None,
            "kg_extraction_completed_at": doc.kg_extraction_completed_at.isoformat() if doc.kg_extraction_completed_at else None,
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£çŠ¶æ€å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail="è·å–æ–‡æ¡£çŠ¶æ€å¤±è´¥")
```

### 5.2 æŸ¥è¯¢çŸ¥è¯†å›¾è°±ç»Ÿè®¡

```python
@router.get("/{kb_id}/documents/{doc_id}/kg/stats")
async def get_document_kg_stats(
    kb_id: int,
    doc_id: int,
    db: Session = Depends(get_db)
):
    """è·å–æ–‡æ¡£çš„çŸ¥è¯†å›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
    try:
        from services.knowledge_graph_service import KnowledgeGraphService
        kg_service = KnowledgeGraphService()
        
        # æŸ¥è¯¢è¯¥æ–‡æ¡£çš„æ‰€æœ‰ä¸‰å…ƒç»„
        triples = db.query(KnowledgeTriple).filter(
            KnowledgeTriple.document_id == doc_id,
            KnowledgeTriple.knowledge_base_id == kb_id
        ).all()
        
        # ç»Ÿè®¡å®ä½“å’Œå…³ç³»
        entities = set()
        relations = {}
        for triple in triples:
            entities.add(triple.subject)
            entities.add(triple.object)
            rel = triple.predicate
            relations[rel] = relations.get(rel, 0) + 1
        
        return {
            "total_triples": len(triples),
            "unique_entities": len(entities),
            "unique_relations": len(relations),
            "top_relations": sorted(relations.items(), key=lambda x: x[1], reverse=True)[:10]
        }
    except Exception as e:
        logger.error(f"è·å–çŸ¥è¯†å›¾è°±ç»Ÿè®¡å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail="è·å–çŸ¥è¯†å›¾è°±ç»Ÿè®¡å¤±è´¥")
```

## å…­ã€å‰ç«¯æ˜¾ç¤ºè®¾è®¡

### 6.1 æ–‡æ¡£åˆ—è¡¨æ˜¾ç¤º

åœ¨ `KnowledgeBasePage.tsx` ä¸­å¢å¼ºæ–‡æ¡£çŠ¶æ€æ˜¾ç¤ºï¼š

```typescript
// agent-ui/src/pages/KnowledgeBasePage.tsx

interface Document {
  // ... ç°æœ‰å­—æ®µ ...
  status: string;
  kg_extraction_status?: string;
  kg_extraction_progress?: {
    total_chunks: number;
    processed: number;
    failed: number;
  };
}

// åœ¨æ–‡æ¡£å¡ç‰‡ä¸­æ˜¾ç¤ºçŠ¶æ€
{doc.status === 'chunked' && doc.kg_extraction_status === 'processing' && (
  <div style={{ marginTop: '8px', padding: '8px', backgroundColor: '#fff7e6', border: '1px solid #ffd591', borderRadius: '4px' }}>
    <p style={{ color: '#d46b08', margin: 0, fontSize: '12px', fontWeight: 'bold' }}>
      ğŸ“Š åˆ†å—å®Œæˆï¼Œæ­£åœ¨æŠ½å–çŸ¥è¯†å›¾è°±...
    </p>
    {doc.kg_extraction_progress && (
      <Progress 
        percent={Math.round((doc.kg_extraction_progress.processed / doc.kg_extraction_progress.total_chunks) * 100)}
        size="small"
        status="active"
        style={{ marginTop: '4px' }}
      />
    )}
    <p style={{ color: '#d46b08', margin: '4px 0 0 0', fontSize: '11px' }}>
      å·²å¤„ç†: {doc.kg_extraction_progress?.processed || 0} / {doc.kg_extraction_progress?.total_chunks || 0}
    </p>
  </div>
)}

{doc.status === 'chunked' && doc.kg_extraction_status === 'completed' && (
  <div style={{ marginTop: '8px', padding: '8px', backgroundColor: '#f6ffed', border: '1px solid #b7eb8f', borderRadius: '4px' }}>
    <p style={{ color: '#52c41a', margin: 0, fontSize: '12px' }}>
      âœ… çŸ¥è¯†å›¾è°±æŠ½å–å®Œæˆ
    </p>
  </div>
)}
```

### 6.2 è½®è¯¢æ›´æ–°çŠ¶æ€

```typescript
// åœ¨æ–‡æ¡£åˆ—è¡¨é¡µé¢æ·»åŠ è½®è¯¢é€»è¾‘
useEffect(() => {
  const interval = setInterval(async () => {
    // åªè½®è¯¢æ­£åœ¨å¤„ç†ä¸­çš„æ–‡æ¡£
    const processingDocs = documents.filter(
      d => d.status === 'processing' || 
           d.status === 'chunked' && d.kg_extraction_status === 'processing'
    );
    
    if (processingDocs.length > 0) {
      // æ‰¹é‡æŸ¥è¯¢çŠ¶æ€
      for (const doc of processingDocs) {
        try {
          const response = await fetch(`${API_BASE}/api/knowledge-base/${selectedKb?.id}/documents/${doc.id}/status`);
          if (response.ok) {
            const status = await response.json();
            // æ›´æ–°æ–‡æ¡£çŠ¶æ€
            setDocuments(prev => prev.map(d => 
              d.id === doc.id ? { ...d, ...status } : d
            ));
          }
        } catch (error) {
          console.error(`æŸ¥è¯¢æ–‡æ¡£ ${doc.id} çŠ¶æ€å¤±è´¥:`, error);
        }
      }
    }
  }, 3000); // æ¯3ç§’è½®è¯¢ä¸€æ¬¡
  
  return () => clearInterval(interval);
}, [documents, selectedKb]);
```

## ä¸ƒã€æ¨¡å‹é›†æˆï¼ˆç»„åˆ1ï¼‰

### 7.1 åˆ›å»º NER + RE æœåŠ¡

```python
# agent-backend/services/ie_model_service.py (æ–°å»º)

from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
from typing import List, Tuple, Dict, Any
import torch

class IEModelService:
    """ä¿¡æ¯æŠ½å–æ¨¡å‹æœåŠ¡ï¼ˆNER + REï¼‰"""
    
    def __init__(self):
        self.ner_model = None
        self.re_model = None
        self.ner_tokenizer = None
        self.re_tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self._load_models()
    
    def _load_models(self):
        """åŠ è½½NERå’ŒREæ¨¡å‹"""
        try:
            # NERæ¨¡å‹ï¼šRoBERTa-CLUE
            ner_model_name = "uer/roberta-base-finetuned-cluener2020-chinese"
            logger.info(f"åŠ è½½NERæ¨¡å‹: {ner_model_name}")
            self.ner_tokenizer = AutoTokenizer.from_pretrained(ner_model_name)
            self.ner_model = AutoModelForTokenClassification.from_pretrained(ner_model_name)
            self.ner_model.to(self.device)
            self.ner_model.eval()
            
            # REæ¨¡å‹ï¼šCasRelï¼ˆéœ€è¦æ ¹æ®å®é™…å¯ç”¨çš„æ¨¡å‹è°ƒæ•´ï¼‰
            # æ³¨æ„ï¼šéœ€è¦æ‰¾åˆ°åˆé€‚çš„ä¸­æ–‡CasRelæƒé‡ï¼Œæˆ–ä½¿ç”¨å…¶ä»–REæ¨¡å‹
            re_model_name = "yubowen-ph/CasRel-bert-base-chinese"  # ç¤ºä¾‹ï¼Œéœ€è¦éªŒè¯
            logger.info(f"åŠ è½½REæ¨¡å‹: {re_model_name}")
            # ... åŠ è½½REæ¨¡å‹ ...
            
            logger.info("ä¿¡æ¯æŠ½å–æ¨¡å‹åŠ è½½å®Œæˆ")
        except Exception as e:
            logger.error(f"åŠ è½½ä¿¡æ¯æŠ½å–æ¨¡å‹å¤±è´¥: {str(e)}")
            raise
    
    def extract_entities(self, text: str) -> List[Dict[str, Any]]:
        """æå–å®ä½“"""
        # ä½¿ç”¨NERæ¨¡å‹æå–å®ä½“
        # ...
        pass
    
    def extract_relations(self, text: str, entities: List[Dict[str, Any]]) -> List[Tuple[str, str, str]]:
        """æå–å…³ç³»ï¼ˆä¸‰å…ƒç»„ï¼‰"""
        # ä½¿ç”¨REæ¨¡å‹æå–å…³ç³»
        # ...
        pass
```

### 7.2 é›†æˆåˆ°çŸ¥è¯†å›¾è°±æœåŠ¡

ä¿®æ”¹ `knowledge_graph_service.py`ï¼Œæ·»åŠ æ¨¡å‹æŠ½å–æ¨¡å¼ï¼š

```python
# agent-backend/services/knowledge_graph_service.py

KG_EXTRACT_MODE = os.getenv("KG_EXTRACT_MODE", "hybrid").lower()  # llm / rule / hybrid / model

class KnowledgeGraphService:
    def __init__(self):
        # ... ç°æœ‰åˆå§‹åŒ– ...
        self.ie_model_service = None
        if KG_EXTRACT_MODE == "model":
            try:
                from services.ie_model_service import IEModelService
                self.ie_model_service = IEModelService()
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½IEæ¨¡å‹ï¼Œå°†å›é€€åˆ°è§„åˆ™/LLMæ¨¡å¼: {str(e)}")
    
    def extract_entities_and_relations(self, ...):
        """æå–å®ä½“å’Œå…³ç³»"""
        if KG_EXTRACT_MODE == "model" and self.ie_model_service:
            # ä½¿ç”¨ä¸“ç”¨æ¨¡å‹æŠ½å–
            entities = self.ie_model_service.extract_entities(text)
            triples = self.ie_model_service.extract_relations(text, entities)
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            # ...
        else:
            # ä½¿ç”¨ç°æœ‰é€»è¾‘ï¼ˆè§„åˆ™/LLM/æ··åˆï¼‰
            # ...
```

## å…«ã€å®æ–½æ­¥éª¤

1. **æ•°æ®åº“è¿ç§»**ï¼šæ·»åŠ æ–°å­—æ®µåˆ° `Document` å’Œ `DocumentChunk` è¡¨
2. **åå°å·¥ä½œå™¨**ï¼šåˆ›å»º `kg_extraction_worker.py`
3. **æœåŠ¡ä¿®æ”¹**ï¼šä¿®æ”¹ `knowledge_base_service.py` çš„ `_process_document_sync`
4. **APIæ¥å£**ï¼šæ·»åŠ çŠ¶æ€æŸ¥è¯¢æ¥å£
5. **å‰ç«¯æ˜¾ç¤º**ï¼šæ›´æ–°æ–‡æ¡£åˆ—è¡¨å’ŒçŠ¶æ€æ˜¾ç¤º
6. **æ¨¡å‹é›†æˆ**ï¼šåˆ›å»º `ie_model_service.py` å¹¶é›†æˆåˆ°çŸ¥è¯†å›¾è°±æœåŠ¡
7. **æµ‹è¯•éªŒè¯**ï¼šæµ‹è¯•å®Œæ•´æµç¨‹

## ä¹ã€é…ç½®é¡¹

```bash
# .env æ–‡ä»¶æ–°å¢é…ç½®

# çŸ¥è¯†å›¾è°±æŠ½å–æ¨¡å¼ï¼šllm / rule / hybrid / model
KG_EXTRACT_MODE=model

# åå°å·¥ä½œå™¨çº¿ç¨‹æ•°
KG_EXTRACTION_WORKERS=2

# æ˜¯å¦å¯ç”¨ä¸‰å…ƒç»„æŠ½å–
EXTRACT_TRIPLES_ENABLED=true
```

## åã€æ³¨æ„äº‹é¡¹

1. **çº¿ç¨‹å®‰å…¨**ï¼šç¡®ä¿æ•°æ®åº“ä¼šè¯åœ¨åå°çº¿ç¨‹ä¸­æ­£ç¡®åˆ›å»ºå’Œå…³é—­
2. **é”™è¯¯å¤„ç†**ï¼šå•ä¸ªåˆ†å—å¤±è´¥ä¸åº”å½±å“æ•´ä½“ä»»åŠ¡
3. **èµ„æºç®¡ç†**ï¼šæ¨¡å‹åŠ è½½ååº”ä¿æŒå¸¸é©»ï¼Œé¿å…é‡å¤åŠ è½½
4. **è¿›åº¦æ›´æ–°**ï¼šå®šæœŸæäº¤æ•°æ®åº“æ›´æ–°ï¼Œé¿å…é•¿æ—¶é—´äº‹åŠ¡
5. **å‰ç«¯è½®è¯¢**ï¼šåˆç†è®¾ç½®è½®è¯¢é—´éš”ï¼Œé¿å…è¿‡åº¦è¯·æ±‚
