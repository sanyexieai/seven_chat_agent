# -*- coding: utf-8 -*-
import os
import asyncio
from typing import Optional, Dict, Any, AsyncGenerator, List
import json
import httpx
import openai
import anthropic
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic

from config.env import MODEL, MODEL_PROVIDER, TEMPERATURE, BASE_URL, API_KEY
from config.llm_config_manager import llm_config_manager
from utils.log_helper import get_logger

# è·å–loggerå®ä¾‹
logger = get_logger("llm_helper")

class LLMHelper:
    """å¤§æ¨¡å‹åŠ©æ‰‹å•ä¾‹ç±»ï¼Œæ”¯æŒå¤šæ¨¡å‹é…ç½®ã€åŠ¨æ€åˆ‡æ¢ã€ä¾¿æ·è°ƒç”¨ã€‚"""
    _instance: Optional['LLMHelper'] = None
    _initialized: bool = False

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if not self._initialized:
            self._config = {}
            self._openai_client = None
            self._anthropic_client = None
            self._ollama_base_url = None
            self._initialized = True

    def setup(self, llm_config: Optional[Dict[str, Any]] = None, **kwargs) -> 'LLMHelper':
        """
        é…ç½®å¹¶åˆå§‹åŒ–å¤§æ¨¡å‹
        
        Args:
            llm_config: å¯é€‰çš„LLMé…ç½®å­—å…¸ï¼ŒåŒ…å«provider, model_name, api_key, api_baseç­‰
            **kwargs: å…¶ä»–é…ç½®å‚æ•°
        """
        if llm_config:
            # ä½¿ç”¨ä¼ å…¥çš„LLMé…ç½®
            self._config = {
                'model': llm_config.get('model_name', ''),
                'model_provider': llm_config.get('provider', ''),
                'temperature': llm_config.get('config', {}).get('temperature', 0.7),
                'base_url': llm_config.get('api_base', ''),
                'api_key': llm_config.get('api_key', '')
            }
            logger.info(f"ä½¿ç”¨ä¼ å…¥çš„LLMé…ç½®: {llm_config.get('provider')} - {llm_config.get('model_name')}")
        else:
            # ä»æ•°æ®åº“é…ç½®ç®¡ç†å™¨è·å–é…ç½®
            db_config = llm_config_manager.get_default_config()
            
            if db_config:
                # ä½¿ç”¨æ•°æ®åº“é…ç½®
                self._config = {
                    'model': db_config.model_name,
                    'model_provider': db_config.provider,
                    'temperature': db_config.config.get('temperature', 0.7) if db_config.config else 0.7,
                    'base_url': db_config.api_base,
                    'api_key': db_config.api_key
                }
                logger.info(f"ä½¿ç”¨æ•°æ®åº“LLMé…ç½®: {db_config.display_name}")
            else:
                # ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®ä½œä¸ºfallback
                self._config = dict(
                    model=MODEL, 
                    model_provider=MODEL_PROVIDER, 
                    temperature=TEMPERATURE, 
                    base_url=BASE_URL, 
                    api_key=API_KEY
                )
                logger.info("ä½¿ç”¨ç¯å¢ƒå˜é‡LLMé…ç½®")
        
        self._config.update(kwargs)
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        self._init_clients()
        
        return self

    def _init_clients(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        try:
            provider = self._config.get('model_provider')
            
            if provider == 'openai':
                self._openai_client = AsyncOpenAI(
                    api_key=self._config.get('api_key'),
                    base_url=self._config.get('base_url')
                )
                logger.info("OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            elif provider == 'anthropic':
                self._anthropic_client = AsyncAnthropic(
                    api_key=self._config.get('api_key')
                )
                logger.info("Anthropicå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            elif provider == 'ollama':
                self._ollama_base_url = self._config.get('base_url', 'http://localhost:11434')
                logger.info(f"Ollamaå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ŒåŸºç¡€URL: {self._ollama_base_url}")
            elif provider == 'deepseek':
                self._deepseek_base_url = self._config.get('base_url', 'https://api.deepseek.com')
                self._deepseek_api_key = self._config.get('api_key', '')
                logger.info(f"DeepSeekå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ŒåŸºç¡€URL: {self._deepseek_base_url}")
            else:
                raise Exception(f"ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†: {provider}")
                
        except Exception as e:
            logger.error(f"LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise

    async def call(self, messages, **kwargs):
        """
        å¼‚æ­¥è°ƒç”¨å¤§æ¨¡å‹
        :param messages: èŠå¤©æ¶ˆæ¯ï¼ˆlist/dict/strï¼‰
        :param kwargs: å…¶ä»–å‚æ•°
        :return: å¤§æ¨¡å‹è¾“å‡º
        """
        try:
            # å¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥
            if isinstance(messages, str):
                # å­—ç¬¦ä¸²è¾“å…¥ï¼Œè½¬æ¢ä¸ºæ¶ˆæ¯æ ¼å¼
                messages = [{"role": "user", "content": messages}]
            elif isinstance(messages, list):
                # ç¡®ä¿æ¶ˆæ¯æ ¼å¼æ­£ç¡®
                formatted_messages = []
                for msg in messages:
                    if isinstance(msg, dict):
                        formatted_messages.append(msg)
                    elif isinstance(msg, str):
                        formatted_messages.append({"role": "user", "content": msg})
                    else:
                        formatted_messages.append({"role": "user", "content": str(msg)})
                messages = formatted_messages
            
            # è®°å½•å¯¹è¯å†…å®¹åˆ°æ—¥å¿—
            logger.info(f"LLMè°ƒç”¨ - æ¨¡å‹: {self._config.get('model_provider')}/{self._config.get('model')}")
            logger.info(f"LLMè¾“å…¥æ¶ˆæ¯: {json.dumps(messages, ensure_ascii=False, indent=2)}")
            
            # è°ƒç”¨å¯¹åº”çš„LLM
            provider = self._config.get('model_provider')
            if provider == 'openai':
                response = await self._call_openai(messages, **kwargs)
            elif provider == 'anthropic':
                response = await self._call_anthropic(messages, **kwargs)
            elif provider == 'ollama':
                response = await self._call_ollama(messages, **kwargs)
            elif provider == 'deepseek':
                response = await self._call_deepseek(messages, **kwargs)
            else:
                raise Exception(f"ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†: {provider}")
            
            # è®°å½•å“åº”åˆ°æ—¥å¿—
            logger.info(f"LLMå“åº”: {response}")
            
            return response
                
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    async def _call_openai(self, messages: List[Dict], **kwargs) -> str:
        """è°ƒç”¨OpenAI API"""
        try:
            if not self._openai_client:
                raise Exception("OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
            # ä¼˜å…ˆä»æ•°æ®åº“é…ç½®è·å–tokenå€¼ï¼Œç„¶åæ˜¯kwargså‚æ•°ï¼Œæœ€åæ˜¯é»˜è®¤å€¼
            db_max_tokens = self._config.get('config', {}).get('max_tokens')
            max_tokens = kwargs.get('max_tokens') or db_max_tokens or 1000
            
            # åˆå¹¶é…ç½®å‚æ•°
            params = {
                "model": self._config.get('model', 'gpt-3.5-turbo'),
                "messages": messages,
                "temperature": self._config.get('temperature', 0.7),
                "max_tokens": max_tokens,
                **kwargs
            }
            
            response = await self._openai_client.chat.completions.create(**params)
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"OpenAIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    async def _call_anthropic(self, messages: List[Dict], **kwargs) -> str:
        """è°ƒç”¨Anthropic API"""
        try:
            if not self._anthropic_client:
                raise Exception("Anthropicå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            system_message = ""
            user_messages = []
            
            for msg in messages:
                if msg.get('role') == 'system':
                    system_message = msg.get('content', '')
                elif msg.get('role') == 'user':
                    user_messages.append(msg.get('content', ''))
                elif msg.get('role') == 'assistant':
                    # Anthropicä¸æ”¯æŒassistantæ¶ˆæ¯ï¼Œè·³è¿‡
                    continue
            
            # åˆå¹¶ç”¨æˆ·æ¶ˆæ¯
            user_content = "\n".join(user_messages)
            
            # ä¼˜å…ˆä»æ•°æ®åº“é…ç½®è·å–tokenå€¼ï¼Œç„¶åæ˜¯kwargså‚æ•°ï¼Œæœ€åæ˜¯é»˜è®¤å€¼
            db_max_tokens = self._config.get('config', {}).get('max_tokens')
            max_tokens = kwargs.get('max_tokens') or db_max_tokens or 1000
            
            # è°ƒç”¨API
            params = {
                "model": self._config.get('model', 'claude-3-sonnet-20240229'),
                "max_tokens": max_tokens,
                "temperature": self._config.get('temperature', 0.7),
                **kwargs
            }
            
            if system_message:
                params["system"] = system_message
            
            response = await self._anthropic_client.messages.create(
                messages=[{"role": "user", "content": user_content}],
                **params
            )
            
            return response.content[0].text
            
        except Exception as e:
            logger.error(f"Anthropicè°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    async def _call_ollama(self, messages: List[Dict], **kwargs) -> str:
        """è°ƒç”¨Ollama API"""
        try:
            if not self._ollama_base_url:
                raise Exception("Ollamaå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
            # ä¼˜å…ˆä»æ•°æ®åº“é…ç½®è·å–tokenå€¼ï¼Œç„¶åæ˜¯kwargså‚æ•°ï¼Œæœ€åæ˜¯é»˜è®¤å€¼
            db_max_tokens = self._config.get('config', {}).get('max_tokens')
            max_tokens = kwargs.get('max_tokens') or db_max_tokens or 512
            max_tokens = max(64, min(int(max_tokens), 512))
            data = {
                "model": self._config.get('model', 'qwen3:8b'),
                "messages": messages,
                "stream": False,
                "keep_alive": "5m",
                "options": {
                    "temperature": self._config.get('temperature', 0.7),
                    "num_predict": max_tokens,
                    **kwargs.get('options', {})
                }
            }
            
            logger.info(f"Ollamaè¯·æ±‚URL: {self._ollama_base_url}/api/chat")
            safe_preview = json.dumps({**data, 'messages': '[omitted for brevity]'}, ensure_ascii=False)
            logger.info(f"Ollamaè¯·æ±‚æ•°æ®(ç®€è¦): {safe_preview}")
            logger.info(f"Ollamaé…ç½®: {self._config}")
            
            # ç›´æ¥ä½¿ç”¨åŒæ­¥requestsåº“ï¼Œé¿å…å¼‚æ­¥é—®é¢˜
            import time
            import requests
            start_time = time.time()
            
            logger.info("ä½¿ç”¨åŒæ­¥requestså‘é€Ollamaè¯·æ±‚...")
            
            try:
                response = requests.post(
                    f"{self._ollama_base_url}/api/chat",
                    json=data,
                    headers={"Content-Type": "application/json"},
                    timeout=30
                )
                
                end_time = time.time()
                logger.info(f"è¯·æ±‚å®Œæˆæ—¶é—´: {end_time}")
                logger.info(f"è¯·æ±‚è€—æ—¶: {end_time - start_time:.2f}ç§’")
                logger.info(f"Ollamaå“åº”çŠ¶æ€ç : {response.status_code}")
                logger.info(f"Ollamaå“åº”å¤´: {dict(response.headers)}")
                
                response.raise_for_status()
                
                result = response.json()
                logger.info(f"Ollamaå“åº”å†…å®¹: {json.dumps(result, ensure_ascii=False, indent=2)}")
                
                content = result.get('message', {}).get('content', '')
                logger.info(f"æå–çš„å†…å®¹: {content}")
                return content
                
            except requests.exceptions.Timeout:
                logger.error("åŒæ­¥è¯·æ±‚è¶…æ—¶")
                raise
            except requests.exceptions.RequestException as e:
                logger.error(f"åŒæ­¥è¯·æ±‚å¤±è´¥: {str(e)}")
                raise
                
        except httpx.HTTPStatusError as e:
            logger.error(f"Ollama HTTPé”™è¯¯: {e.response.status_code} - {e.response.text}")
            raise
        except httpx.RequestError as e:
            logger.error(f"Ollamaè¯·æ±‚é”™è¯¯: {str(e)}")
            raise
        except json.JSONDecodeError as e:
            logger.error(f"Ollamaå“åº”JSONè§£æé”™è¯¯: {str(e)}")
            logger.error(f"å“åº”å†…å®¹: {response.text if 'response' in locals() else 'N/A'}")
            raise
        except Exception as e:
            logger.error(f"Ollamaè°ƒç”¨å¤±è´¥: {str(e)}")
            logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
            
            # åŒæ­¥è¯·æ±‚å·²ç»å¤±è´¥ï¼Œè®°å½•é”™è¯¯
            logger.error("åŒæ­¥è¯·æ±‚å¤±è´¥ï¼Œæ— æ³•è·å–Ollamaå“åº”")

    async def call_stream(self, messages, **kwargs) -> AsyncGenerator[str, None]:
        """
        æµå¼è°ƒç”¨å¤§æ¨¡å‹
        :param messages: èŠå¤©æ¶ˆæ¯
        :param kwargs: å…¶ä»–å‚æ•°
        :yield: æµå¼å“åº”ç‰‡æ®µ
        """
        try:
            # å¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥
            if isinstance(messages, str):
                messages = [{"role": "user", "content": messages}]
            elif isinstance(messages, list):
                formatted_messages = []
                for msg in messages:
                    if isinstance(msg, dict):
                        formatted_messages.append(msg)
                    elif isinstance(msg, str):
                        formatted_messages.append({"role": "user", "content": msg})
                    else:
                        formatted_messages.append({"role": "user", "content": str(msg)})
                messages = formatted_messages
            
            # è®°å½•æµå¼å¯¹è¯å†…å®¹åˆ°æ—¥å¿—
            logger.info(f"LLMæµå¼è°ƒç”¨ - æ¨¡å‹: {self._config.get('model_provider')}/{self._config.get('model')}")
            logger.info(f"LLMæµå¼è¾“å…¥æ¶ˆæ¯: {json.dumps(messages, ensure_ascii=False, indent=2)}")
            
            # è°ƒç”¨å¯¹åº”çš„æµå¼LLM
            provider = self._config.get('model_provider')
            full_response = ""
            
            if provider == 'openai':
                async for chunk in self._call_openai_stream(messages, **kwargs):
                    full_response += chunk
                    yield chunk
            elif provider == 'anthropic':
                async for chunk in self._call_anthropic_stream(messages, **kwargs):
                    full_response += chunk
                    yield chunk
            elif provider == 'ollama':
                async for chunk in self._call_ollama_stream(messages, **kwargs):
                    full_response += chunk
                    yield chunk
            elif provider == 'deepseek':
                async for chunk in self._call_deepseek_stream(messages, **kwargs):
                    full_response += chunk
                    yield chunk
            else:
                raise Exception(f"ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†: {provider}")
            
            # è®°å½•å®Œæ•´å“åº”åˆ°æ—¥å¿—
            logger.info(f"LLMæµå¼å“åº”å®Œæ•´å†…å®¹: {full_response}")
                    
        except Exception as e:
            logger.error(f"LLMæµå¼è°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    async def _call_openai_stream(self, messages: List[Dict], **kwargs) -> AsyncGenerator[str, None]:
        """æµå¼è°ƒç”¨OpenAI API"""
        try:
            if not self._openai_client:
                raise Exception("OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
            # ä¼˜å…ˆä»æ•°æ®åº“é…ç½®è·å–tokenå€¼ï¼Œç„¶åæ˜¯kwargså‚æ•°ï¼Œæœ€åæ˜¯é»˜è®¤å€¼
            db_max_tokens = self._config.get('config', {}).get('max_tokens')
            max_tokens = kwargs.get('max_tokens') or db_max_tokens or 1000
            
            params = {
                "model": self._config.get('model', 'gpt-3.5-turbo'),
                "messages": messages,
                "temperature": self._config.get('temperature', 0.7),
                "max_tokens": max_tokens,
                "stream": True,
                **kwargs
            }
            
            stream = await self._openai_client.chat.completions.create(**params)
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            logger.error(f"OpenAIæµå¼è°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    async def _call_anthropic_stream(self, messages: List[Dict], **kwargs) -> AsyncGenerator[str, None]:
        """æµå¼è°ƒç”¨Anthropic API"""
        try:
            if not self._anthropic_client:
                raise Exception("Anthropicå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            system_message = ""
            user_messages = []
            
            for msg in messages:
                if msg.get('role') == 'system':
                    system_message = msg.get('content', '')
                elif msg.get('role') == 'user':
                    user_messages.append(msg.get('content', ''))
                elif msg.get('role') == 'assistant':
                    continue
            
            user_content = "\n".join(user_messages)
            
            # ä¼˜å…ˆä»æ•°æ®åº“é…ç½®è·å–tokenå€¼ï¼Œç„¶åæ˜¯kwargså‚æ•°ï¼Œæœ€åæ˜¯é»˜è®¤å€¼
            db_max_tokens = self._config.get('config', {}).get('max_tokens')
            max_tokens = kwargs.get('max_tokens') or db_max_tokens or 1000
            
            params = {
                "model": self._config.get('model', 'claude-3-sonnet-20240229'),
                "max_tokens": max_tokens,
                "temperature": self._config.get('temperature', 0.7),
                **kwargs
            }
            
            if system_message:
                params["system"] = system_message
            
            stream = await self._anthropic_client.messages.create(
                messages=[{"role": "user", "content": user_content}],
                stream=True,
                **params
            )
            
            async for chunk in stream:
                if chunk.type == "content_block_delta":
                    yield chunk.delta.text
                    
        except Exception as e:
            logger.error(f"Anthropicæµå¼è°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    async def _call_ollama_stream(self, messages: List[Dict], **kwargs) -> AsyncGenerator[str, None]:
        """æµå¼è°ƒç”¨Ollama API"""
        try:
            if not self._ollama_base_url:
                raise Exception("Ollamaå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
            # ä¼˜å…ˆä»æ•°æ®åº“é…ç½®è·å–tokenå€¼ï¼Œç„¶åæ˜¯kwargså‚æ•°ï¼Œæœ€åæ˜¯é»˜è®¤å€¼
            db_max_tokens = self._config.get('config', {}).get('max_tokens')
            max_tokens = kwargs.get('max_tokens') or db_max_tokens or 512
            max_tokens = max(64, min(int(max_tokens), 512))
            data = {
                "model": self._config.get('model', 'qwen3:8b'),
                "messages": messages,
                "stream": True,
                "keep_alive": "5m",
                "options": {
                    "temperature": self._config.get('temperature', 0.7),
                    "num_predict": max_tokens,
                    **kwargs.get('options', {})
                }
            }
            
            logger.info(f"Ollamaæµå¼è¯·æ±‚URL: {self._ollama_base_url}/api/chat")
            safe_preview = json.dumps({**data, 'messages': '[omitted for brevity]'}, ensure_ascii=False)
            logger.info(f"Ollamaæµå¼è¯·æ±‚æ•°æ®(ç®€è¦): {safe_preview}")
            
            # ä½¿ç”¨åŒæ­¥requestsè¿›è¡Œæµå¼è¯·æ±‚ï¼Œé¿å…å¼‚æ­¥é—®é¢˜
            import requests
            logger.info("ä½¿ç”¨åŒæ­¥requestså‘é€Ollamaæµå¼è¯·æ±‚...")
            
            try:
                response = requests.post(
                    f"{self._ollama_base_url}/api/chat",
                    json=data,
                    headers={"Content-Type": "application/json"},
                    timeout=30,
                    stream=True
                )
                
                logger.info(f"Ollamaæµå¼å“åº”çŠ¶æ€ç : {response.status_code}")
                logger.info(f"Ollamaæµå¼å“åº”å¤´: {dict(response.headers)}")
                
                response.raise_for_status()
                
                # å¤„ç†æµå¼å“åº”
                for line in response.iter_lines():
                    if line:
                        try:
                            line_text = line.decode('utf-8').strip()
                            if line_text:
                                logger.debug(f"æ”¶åˆ°æµå¼æ•°æ®è¡Œ: {line_text}")
                                chunk_data = json.loads(line_text)
                                if 'message' in chunk_data and 'content' in chunk_data['message']:
                                    content = chunk_data['message']['content']
                                    logger.debug(f"æµå¼å†…å®¹ç‰‡æ®µ: {content}")
                                    yield content
                        except json.JSONDecodeError as e:
                            logger.warning(f"æµå¼æ•°æ®JSONè§£æå¤±è´¥: {str(e)}, æ•°æ®è¡Œ: {line_text}")
                            continue
                        except Exception as e:
                            logger.warning(f"å¤„ç†æµå¼æ•°æ®å¤±è´¥: {str(e)}")
                            continue
                            
            except requests.exceptions.Timeout:
                logger.error("åŒæ­¥æµå¼è¯·æ±‚è¶…æ—¶")
                raise
            except requests.exceptions.RequestException as e:
                logger.error(f"åŒæ­¥æµå¼è¯·æ±‚å¤±è´¥: {str(e)}")
                raise
                                
        except httpx.HTTPStatusError as e:
            logger.error(f"Ollamaæµå¼HTTPé”™è¯¯: {e.response.status_code} - {e.response.text}")
            raise
        except httpx.RequestError as e:
            logger.error(f"Ollamaæµå¼è¯·æ±‚é”™è¯¯: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"Ollamaæµå¼è°ƒç”¨å¤±è´¥: {str(e)}")
            logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
            raise

    async def _call_deepseek(self, messages: List[Dict], **kwargs) -> str:
        """è°ƒç”¨DeepSeek API"""
        try:
            if not self._deepseek_base_url or not self._deepseek_api_key:
                raise Exception("DeepSeekå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
            # æ„å»ºè¯·æ±‚æ•°æ®
            data = {
                "model": self._config.get('model', 'deepseek-chat'),
                "messages": messages,
                "temperature": self._config.get('temperature', 0.7),
                "max_tokens": kwargs.get('max_tokens', 1000),
                **kwargs
            }
            
            # å‘é€è¯·æ±‚
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{self._deepseek_base_url}/v1/chat/completions",
                    json=data,
                    headers={
                        "Authorization": f"Bearer {self._deepseek_api_key}",
                        "Content-Type": "application/json"
                    },
                    timeout=60
                )
                response.raise_for_status()
                
                result = response.json()
                return result.get('choices', [{}])[0].get('message', {}).get('content', '')
                
        except Exception as e:
            logger.error(f"DeepSeekè°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    async def _call_deepseek_stream(self, messages: List[Dict], **kwargs) -> AsyncGenerator[str, None]:
        """æµå¼è°ƒç”¨DeepSeek API"""
        try:
            if not self._deepseek_base_url or not self._deepseek_api_key:
                raise Exception("DeepSeekå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
            # æ„å»ºè¯·æ±‚æ•°æ®
            data = {
                "model": self._config.get('model', 'deepseek-chat'),
                "messages": messages,
                "temperature": self._config.get('temperature', 0.7),
                "max_tokens": kwargs.get('max_tokens', 1000),
                "stream": True,
                **kwargs
            }
            
            # å‘é€æµå¼è¯·æ±‚
            async with httpx.AsyncClient() as client:
                async with client.stream(
                    "POST",
                    f"{self._deepseek_base_url}/v1/chat/completions",
                    json=data,
                    headers={
                        "Authorization": f"Bearer {self._deepseek_api_key}",
                        "Content-Type": "application/json"
                    },
                    timeout=60
                ) as response:
                    response.raise_for_status()
                    
                    async for line in response.aiter_lines():
                        if line.strip() and line.startswith('data: '):
                            try:
                                chunk_data = json.loads(line[6:])  # ç§»é™¤ 'data: ' å‰ç¼€
                                if chunk_data.get('choices') and chunk_data['choices'][0].get('delta', {}).get('content'):
                                    yield chunk_data['choices'][0]['delta']['content']
                            except json.JSONDecodeError:
                                continue
                                
        except Exception as e:
            logger.error(f"DeepSeekæµå¼è°ƒç”¨å¤±è´¥: {str(e)}")
            raise

    def switch_model(self, model: str, **kwargs):
        """
        åŠ¨æ€åˆ‡æ¢æ¨¡å‹
        """
        cfg = self._config.copy()
        cfg.update(model=model)
        cfg.update(kwargs)
        self._config = cfg
        self._init_clients()
        return self
    
    def switch_config(self, config_name: str):
        """
        åˆ‡æ¢åˆ°æŒ‡å®šçš„LLMé…ç½®
        """
        config = llm_config_manager.get_config(config_name)
        if config:
            self._config = {
                'model': config.model_name,
                'model_provider': config.provider,
                'temperature': config.config.get('temperature', 0.7) if config.config else 0.7,
                'base_url': config.api_base,
                'api_key': config.api_key
            }
            self._init_clients()
            logger.info(f"åˆ‡æ¢åˆ°LLMé…ç½®: {config.display_name}")
        else:
            logger.error(f"æœªæ‰¾åˆ°LLMé…ç½®: {config_name}")
    
    def refresh_config(self):
        """
        åˆ·æ–°é…ç½®ï¼ˆä»æ•°æ®åº“é‡æ–°åŠ è½½ï¼‰
        """
        llm_config_manager.refresh_config()
        self.setup()

    def get_config(self) -> dict:
        """è·å–å½“å‰æ¨¡å‹é…ç½®"""
        return self._config.copy()

    def filter_think_content(self, text: str) -> str:
        """è¿‡æ»¤æ‰<think>æ ‡ç­¾å†…çš„æ€è€ƒå†…å®¹"""
        import re
        # ç§»é™¤<think>...</think>æ ‡ç­¾åŠå…¶å†…å®¹
        filtered_text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        filtered_text = re.sub(r'\s+', ' ', filtered_text).strip()
        return filtered_text

# è·å–å•ä¾‹
def get_llm_helper(llm_config: Optional[Dict[str, Any]] = None):
    """è·å–LLMåŠ©æ‰‹å®ä¾‹
    
    Args:
        llm_config: å¯é€‰çš„LLMé…ç½®å­—å…¸
    """
    llm_helper = LLMHelper()
    llm_helper.setup(llm_config=llm_config)
    return llm_helper

def get_llm(model: str = None, **kwargs):
    """
    è·å–å¤§æ¨¡å‹å®ä¾‹çš„ä¾¿æ·å‡½æ•°
    :param model: æŒ‡å®šæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
    :param kwargs: å…¶ä»–å‚æ•°
    :return: å¤§æ¨¡å‹å®ä¾‹
    """
    helper = get_llm_helper()
    if model:
        return helper.switch_model(model, **kwargs)
    return helper

def setup_llm(**kwargs) -> LLMHelper:
    """
    é…ç½®å¹¶åˆå§‹åŒ–å¤§æ¨¡å‹çš„ä¾¿æ·å‡½æ•°
    :param kwargs: é…ç½®å‚æ•°
    :return: LLMHelperå®ä¾‹
    """
    #å°è¯•ä»ç¯å¢ƒå˜é‡è·å–å‚æ•°
    if "MODEL_PROVIDER" in os.environ:
        kwargs["model_provider"] = os.environ["MODEL_PROVIDER"]
    if "MODEL" in os.environ:
        kwargs["model"] = os.environ["MODEL"]
    if "TEMPERATURE" in os.environ:
        kwargs["temperature"] = float(os.environ["TEMPERATURE"])
    
    llm_helper = LLMHelper()
    return llm_helper.setup(**kwargs)

if __name__ == "__main__":
    async def test_llm_helper():
        """æµ‹è¯•LLMåŠ©æ‰‹"""
        print("ğŸ§ª æµ‹è¯•LLMåŠ©æ‰‹...")
        
        # åˆ›å»ºLLMåŠ©æ‰‹
        llm_helper = get_llm_helper()
        
        # æµ‹è¯•è°ƒç”¨
        test_prompts = [
            "ä½ å¥½",
            "è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
            "ç°åœ¨å‡ ç‚¹äº†ï¼Ÿ",
            "æœç´¢äººå·¥æ™ºèƒ½ä¿¡æ¯"
        ]
        
        for prompt in test_prompts:
            print(f"\nğŸ“ è¾“å…¥: {prompt}")
            try:
                response = await llm_helper.call(prompt)
                print(f"ğŸ¤– è¾“å‡º: {response}")
            except Exception as e:
                print(f"âŒ é”™è¯¯: {str(e)}")
        
        print("\nâœ… LLMåŠ©æ‰‹æµ‹è¯•å®Œæˆ")

    asyncio.run(test_llm_helper())